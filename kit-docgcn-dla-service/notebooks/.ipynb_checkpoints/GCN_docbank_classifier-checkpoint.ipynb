{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQpJTV7RSVZW",
    "tags": []
   },
   "source": [
    "## Environment Setup\n",
    "Import key libraries and working envorinments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-GlywkSFegL"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "979OUro5Eac3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#from transformers.configuration_bert import BertConfig\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I9sVem7yb4MN"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# import pandas as pd\n",
    "\n",
    "# # Authenticate\n",
    "# drive = None\n",
    "# def authenticate():\n",
    "#   global drive\n",
    "  \n",
    "#   auth.authenticate_user()\n",
    "#   gauth = GoogleAuth()\n",
    "#   gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#   drive = GoogleDrive(gauth)\n",
    "\n",
    "# #Download files\n",
    "# def downloadFiles(fileIds):\n",
    "#   authenticate()\n",
    "  \n",
    "#   for fileId in fileIds:    \n",
    "    \n",
    "#     downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
    "#     downloaded.GetContentFile(fileId[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Sd2-rgBQp269"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7TWqvbTppX_"
   },
   "source": [
    "##Get Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pb5R9jPabbKD"
   },
   "outputs": [],
   "source": [
    "# #Do not downloading training and validation dataset at same time \n",
    "# try:\n",
    "#   _ = open(\"testing_dataset.pkl\", \"r\")\n",
    "# except:\n",
    "#   downloadFiles([[\"testing_dataset.pkl\", \"1fktW64hxcjCXreMTv_2pAxSe4Nt083z3\"]])\n",
    "\n",
    "# try:\n",
    "#   _ = open(\"training_dataset.pkl\", \"r\")\n",
    "# except:\n",
    "#   downloadFiles([[\"training_dataset.pkl\", \"1td4mF-QxrwKF125xR5DWflGqcwn0z1LP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "t_0-NGiEViK1"
   },
   "outputs": [],
   "source": [
    "# #Download file if not existing\n",
    "# try:\n",
    "#   _ = open(\"visual_train.pkl\", \"r\")\n",
    "# except:\n",
    "#   downloadFiles([[\"visual_train.pkl\", \"1-1iI14bgFX8QbhA1uPjI7VA6kfIhPhxM\"]])\n",
    "\n",
    "# try:\n",
    "#   _ = open(\"visual_test.pkl\", \"r\")\n",
    "# except:\n",
    "#   downloadFiles([[\"visual_test.pkl\", \"1sEVX0nd09MfvgwrPc4KxWrClDIJpQOoz\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../docbank_data/docbank_training_parent_child.pkl\", \"rb\") as fp:\n",
    "    train_pc_dict = pickle.load(fp) \n",
    " \n",
    "with open(\"../docbank_data/docbank_testing_parent_child.pkl\", \"rb\") as fp:\n",
    "    test_pc_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pc_dict.keys()), len(train_dw_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_pc_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'195.tar_1610.03346.gz_hj_19_ori'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_pc_dict.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'box', 'category', 'text', 'relations', 'bert_large_emb', 'gcn_bert_large'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pc_dict['1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori']['objects']['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pc_dict['1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori']['objects']['0']['bert_large_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pc_dict['1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori']['objects']['0']['gcn_bert_large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../docbank_data/docbank_training_distance_weighted.pkl\", \"rb\") as fp:\n",
    "    train_dw_dict = pickle.load(fp) \n",
    " \n",
    "with open(\"../docbank_data/docbank_testing_distance_weighted.pkl\", \"rb\") as fp:\n",
    "    test_dw_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density', 'text_number', 'char_density', 'char_number', 'parsing_level1', 'parsing_level2', 'near_gap', 'gcn_token_number'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dw_dict['1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori']['objects']['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "img_ls = sorted(os.listdir('../docbank_data/docbank_training_data/images'))\n",
    "visual_features_train_list = []\n",
    "for i in range(len(img_ls[:20])):\n",
    "      file_name = img_ls[i][:-4]\n",
    "      path = '../docbank_data/visual_features/object_train2/'+file_name+'.json'\n",
    "      # print(path)\n",
    "      with open(path,'r') as file_object:\n",
    "        visual_features = json.load(file_object)\n",
    "        visual_features_train_list.append(visual_features)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2048\n"
     ]
    }
   ],
   "source": [
    "print(len(visual_features_train_list), len(visual_features_train_list[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual_features_train_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "img_ls = sorted(os.listdir('../docbank_data/docbank_testing_data/images'))\n",
    "visual_features_test_list = []\n",
    "for i in range(len(img_ls[:20])):\n",
    "      file_name = img_ls[i][:-4]\n",
    "      path = '../docbank_data/visual_features/object_test2/'+file_name+'.json'\n",
    "      # print(path)\n",
    "      with open(path,'r') as file_object:\n",
    "        visual_features = json.load(file_object)\n",
    "        visual_features_test_list.append(visual_features)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2048\n"
     ]
    }
   ],
   "source": [
    "print(len(visual_features_test_list), len(visual_features_test_list[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual_features_test_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k,p in enumerate(train_dw_dict):\n",
    "    for i, j in enumerate(train_dw_dict[p]['objects']):\n",
    "        train_dw_dict[p]['objects'][j]['visual_embeddings'] = visual_features_train_list[k][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density', 'text_number', 'char_density', 'char_number', 'parsing_level1', 'parsing_level2', 'near_gap', 'gcn_token_number', 'visual_embeddings'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dw_dict['1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori']['objects']['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dw_dict['1.tar_1401.0001.gz_infoingames_without_metric_arxiv_0_ori']['objects']['0']['visual_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k,p in enumerate(test_dw_dict):\n",
    "    for i, j in enumerate(test_dw_dict[p]['objects']):\n",
    "        test_dw_dict[p]['objects'][j]['visual_embeddings'] = visual_features_test_list[k][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density', 'text_number', 'char_density', 'char_number', 'near_gap', 'gcn_token_number', 'visual_embeddings'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dw_dict['195.tar_1610.03346.gz_hj_19_ori']['objects']['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dw_dict['195.tar_1610.03346.gz_hj_19_ori']['objects']['0']['visual_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density', 'text_number', 'char_density', 'char_number', 'near_gap', 'gcn_token_number', 'visual_embeddings'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dw_dict['195.tar_1610.03346.gz_hj_19_ori']['objects']['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dw_dict['195.tar_1610.03346.gz_hj_19_ori']['objects']['0']['gcn_token_number'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dw_dict['195.tar_1610.03346.gz_hj_19_ori']['objects']['0']['visual_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qiF6LKqVuje"
   },
   "outputs": [],
   "source": [
    "# train_visual = pd.read_pickle('visual_train.pkl')\n",
    "# test_visual = pd.read_pickle('visual_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4bGVgzk-HT-"
   },
   "outputs": [],
   "source": [
    "# print(train_visual.shape)\n",
    "# print(test_visual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY7KOu_6Dtzh"
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_pickle('training_dataset.pkl')\n",
    "# df_test = pd.read_pickle('testing_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k5hCBUG-O1x"
   },
   "outputs": [],
   "source": [
    "# print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzwacY0IH7ef"
   },
   "outputs": [],
   "source": [
    "# df_train['visual_embedding'] = test_visual['visual_embedding']\n",
    "# df_test['visual_embedding'] = train_visual['visual_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNZW1muhZXBL"
   },
   "outputs": [],
   "source": [
    "# print(test_visual.shape)\n",
    "# print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DkpX4X0YxgK"
   },
   "outputs": [],
   "source": [
    "# test_visual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2EV5VAOH93J"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdJi7AxkpQFr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_train.head()\n",
    "density_emb_train = df_train['char_density_emb'].tolist()\n",
    "density_emb_test = df_test['char_density_emb'].tolist()\n",
    "number_emb_train = df_train['char_number_emb'].tolist()\n",
    "number_emb_test = df_test['char_number_emb'].tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GljdlwFo-IF"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_train = pd.read_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_train.pkl')\n",
    "df_test = pd.read_pickle('/content/drive/MyDrive/funsd/funsd_object_gcn_visual_density_test.pkl')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wH2iQ_QspxiY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_train['char_density_emb'] = density_emb_train\n",
    "df_train['char_number_emb'] = number_emb_train\n",
    "df_test['char_density_emb'] = density_emb_test\n",
    "df_test['char_number_emb'] = number_emb_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sb1Q5N6LGK7z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dw_list = []\n",
    "for file in train_dw_dict:\n",
    "    for x in train_dw_dict[file]['objects']:\n",
    "      training_dw_list.append(train_dw_dict[file]['objects'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12867"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_dw_list = []\n",
    "for file in test_dw_dict:\n",
    "    for x in test_dw_dict[file]['objects']:\n",
    "      testing_dw_list.append(test_dw_dict[file]['objects'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10508"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_dw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_pc_list = []\n",
    "for file in train_pc_dict:\n",
    "    for x in train_pc_dict[file]['objects']:\n",
    "      training_pc_list.append(train_pc_dict[file]['objects'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12867"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_pc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_pc_list = []\n",
    "for file in test_pc_dict:\n",
    "    for x in test_pc_dict[file]['objects']:\n",
    "      testing_pc_list.append(test_pc_dict[file]['objects'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10508"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_pc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "df_dw_train = DataFrame(training_dw_list)\n",
    "df_dw_test = DataFrame(testing_dw_list)\n",
    "df_pc_train = DataFrame(training_pc_list)\n",
    "df_pc_test = DataFrame(testing_pc_list)\n",
    "\n",
    "# df_dw_train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12867, 15), (12867, 7))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw_train.shape, df_pc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10508, 13), (10508, 7))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw_test.shape, df_pc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density',\n",
       "       'text_number', 'char_density', 'char_number', 'parsing_level1',\n",
       "       'parsing_level2', 'near_gap', 'gcn_token_number', 'visual_embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'box', 'category', 'text', 'relations', 'bert_large_emb',\n",
       "       'gcn_bert_large'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12867, 15), (12867, 3))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dw_train.shape, df_pc_train[['text','bert_large_emb','gcn_bert_large']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>box</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>relations</th>\n",
       "      <th>bert_large_emb</th>\n",
       "      <th>gcn_bert_large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[237, 211, 301, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>Value</td>\n",
       "      <td>{'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...</td>\n",
       "      <td>[-0.319358766078949, 0.01164466142654419, -0.1...</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[308, 211, 332, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>of</td>\n",
       "      <td>{'0': {'id': 243, 'object': 0}, '1': {'id': 24...</td>\n",
       "      <td>[-0.26809656620025635, 0.20610269904136658, -0...</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[339, 211, 472, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>information</td>\n",
       "      <td>{'0': {'id': 486, 'object': 0}, '1': {'id': 48...</td>\n",
       "      <td>[-0.15259815752506256, 0.19541406631469727, 0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[479, 211, 501, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>in</td>\n",
       "      <td>{'0': {'id': 729, 'object': 0}, '1': {'id': 73...</td>\n",
       "      <td>[-0.0863858014345169, 0.19652682542800903, -0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[508, 211, 680, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>noncooperative</td>\n",
       "      <td>{'0': {'id': 972, 'object': 0}, '1': {'id': 97...</td>\n",
       "      <td>[-0.5515077710151672, -0.2693435549736023, -0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   box category            text  \\\n",
       "0   0  [237, 211, 301, 237]    title           Value   \n",
       "1   1  [308, 211, 332, 237]    title              of   \n",
       "2   2  [339, 211, 472, 237]    title     information   \n",
       "3   3  [479, 211, 501, 237]    title              in   \n",
       "4   4  [508, 211, 680, 237]    title  noncooperative   \n",
       "\n",
       "                                           relations  \\\n",
       "0  {'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...   \n",
       "1  {'0': {'id': 243, 'object': 0}, '1': {'id': 24...   \n",
       "2  {'0': {'id': 486, 'object': 0}, '1': {'id': 48...   \n",
       "3  {'0': {'id': 729, 'object': 0}, '1': {'id': 73...   \n",
       "4  {'0': {'id': 972, 'object': 0}, '1': {'id': 97...   \n",
       "\n",
       "                                      bert_large_emb  \\\n",
       "0  [-0.319358766078949, 0.01164466142654419, -0.1...   \n",
       "1  [-0.26809656620025635, 0.20610269904136658, -0...   \n",
       "2  [-0.15259815752506256, 0.19541406631469727, 0....   \n",
       "3  [-0.0863858014345169, 0.19652682542800903, -0....   \n",
       "4  [-0.5515077710151672, -0.2693435549736023, -0....   \n",
       "\n",
       "                                      gcn_bert_large  \n",
       "0  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "1  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "2  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "3  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "4  [0.009847326, -0.00019112365, -0.0008584456, 0...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_dw_train, df_pc_train[['id','bert_large_emb','gcn_bert_large']]\\\n",
    "                    , on=['id','id'], how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12867, 17)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>box</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>relations</th>\n",
       "      <th>gap</th>\n",
       "      <th>text_density</th>\n",
       "      <th>text_number</th>\n",
       "      <th>char_density</th>\n",
       "      <th>char_number</th>\n",
       "      <th>parsing_level1</th>\n",
       "      <th>parsing_level2</th>\n",
       "      <th>near_gap</th>\n",
       "      <th>gcn_token_number</th>\n",
       "      <th>visual_embeddings</th>\n",
       "      <th>bert_large_emb</th>\n",
       "      <th>gcn_bert_large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[237, 211, 301, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>Value</td>\n",
       "      <td>{'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...</td>\n",
       "      <td>{'1': 7, '2': 38, '3': 178, '4': 207, '5': 386...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'39': 3, '188': 4}</td>\n",
       "      <td>[0.028947916, -0.05766789, -0.024570161, 0.132...</td>\n",
       "      <td>[0.0, 1.6673742532730103, 2.7821877002716064, ...</td>\n",
       "      <td>[-0.319358766078949, 0.01164466142654419, -0.1...</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[308, 211, 332, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>of</td>\n",
       "      <td>{'0': {'id': 243, 'object': 0}, '1': {'id': 24...</td>\n",
       "      <td>{'0': 7, '2': 7, '3': 147, '4': 176, '5': 355,...</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'138': 1, '176': 1}</td>\n",
       "      <td>[0.03022929, -0.058581498, -0.02262115, 0.1348...</td>\n",
       "      <td>[0.0, 0.9130946397781372, 0.8139663934707642, ...</td>\n",
       "      <td>[-0.26809656620025635, 0.20610269904136658, -0...</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[339, 211, 472, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>information</td>\n",
       "      <td>{'0': {'id': 486, 'object': 0}, '1': {'id': 48...</td>\n",
       "      <td>{'0': 38, '1': 7, '3': 7, '4': 36, '5': 215, '...</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'76': 4, '213': 5}</td>\n",
       "      <td>[0.029282099, -0.061983045, -0.022321349, 0.12...</td>\n",
       "      <td>[0.0, 1.0808653831481934, 1.1382662057876587, ...</td>\n",
       "      <td>[-0.15259815752506256, 0.19541406631469727, 0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[479, 211, 501, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>in</td>\n",
       "      <td>{'0': {'id': 729, 'object': 0}, '1': {'id': 73...</td>\n",
       "      <td>{'0': 178, '1': 147, '2': 7, '4': 7, '5': 186,...</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'130': 3, '181': 6}</td>\n",
       "      <td>[0.029444845, -0.0605363, -0.027396373, 0.1415...</td>\n",
       "      <td>[0.0, 0.7543953657150269, 2.0874814987182617, ...</td>\n",
       "      <td>[-0.0863858014345169, 0.19652682542800903, -0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[508, 211, 680, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>noncooperative</td>\n",
       "      <td>{'0': {'id': 972, 'object': 0}, '1': {'id': 97...</td>\n",
       "      <td>{'0': 207, '1': 176, '2': 36, '3': 7, '5': 7, ...</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>14</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'92': 1, '136': 1, '226': 1}</td>\n",
       "      <td>[0.03045706, -0.060177967, -0.030296788, 0.146...</td>\n",
       "      <td>[0.0, 1.8353612422943115, 0.5946379899978638, ...</td>\n",
       "      <td>[-0.5515077710151672, -0.2693435549736023, -0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   box category            text  \\\n",
       "0   0  [237, 211, 301, 237]    title           Value   \n",
       "1   1  [308, 211, 332, 237]    title              of   \n",
       "2   2  [339, 211, 472, 237]    title     information   \n",
       "3   3  [479, 211, 501, 237]    title              in   \n",
       "4   4  [508, 211, 680, 237]    title  noncooperative   \n",
       "\n",
       "                                           relations  \\\n",
       "0  {'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...   \n",
       "1  {'0': {'id': 243, 'object': 0}, '1': {'id': 24...   \n",
       "2  {'0': {'id': 486, 'object': 0}, '1': {'id': 48...   \n",
       "3  {'0': {'id': 729, 'object': 0}, '1': {'id': 73...   \n",
       "4  {'0': {'id': 972, 'object': 0}, '1': {'id': 97...   \n",
       "\n",
       "                                                 gap  text_density  \\\n",
       "0  {'1': 7, '2': 38, '3': 178, '4': 207, '5': 386...      0.000601   \n",
       "1  {'0': 7, '2': 7, '3': 147, '4': 176, '5': 355,...      0.001603   \n",
       "2  {'0': 38, '1': 7, '3': 7, '4': 36, '5': 215, '...      0.000289   \n",
       "3  {'0': 178, '1': 147, '2': 7, '4': 7, '5': 186,...      0.001748   \n",
       "4  {'0': 207, '1': 176, '2': 36, '3': 7, '5': 7, ...      0.000224   \n",
       "\n",
       "   text_number  char_density  char_number parsing_level1 parsing_level2  \\\n",
       "0            1      0.003005            5             []             []   \n",
       "1            1      0.003205            2             []             []   \n",
       "2            1      0.003181           11             []             []   \n",
       "3            1      0.003497            2             []             []   \n",
       "4            1      0.003131           14             []             []   \n",
       "\n",
       "                        near_gap  \\\n",
       "0            {'39': 3, '188': 4}   \n",
       "1           {'138': 1, '176': 1}   \n",
       "2            {'76': 4, '213': 5}   \n",
       "3           {'130': 3, '181': 6}   \n",
       "4  {'92': 1, '136': 1, '226': 1}   \n",
       "\n",
       "                                    gcn_token_number  \\\n",
       "0  [0.028947916, -0.05766789, -0.024570161, 0.132...   \n",
       "1  [0.03022929, -0.058581498, -0.02262115, 0.1348...   \n",
       "2  [0.029282099, -0.061983045, -0.022321349, 0.12...   \n",
       "3  [0.029444845, -0.0605363, -0.027396373, 0.1415...   \n",
       "4  [0.03045706, -0.060177967, -0.030296788, 0.146...   \n",
       "\n",
       "                                   visual_embeddings  \\\n",
       "0  [0.0, 1.6673742532730103, 2.7821877002716064, ...   \n",
       "1  [0.0, 0.9130946397781372, 0.8139663934707642, ...   \n",
       "2  [0.0, 1.0808653831481934, 1.1382662057876587, ...   \n",
       "3  [0.0, 0.7543953657150269, 2.0874814987182617, ...   \n",
       "4  [0.0, 1.8353612422943115, 0.5946379899978638, ...   \n",
       "\n",
       "                                      bert_large_emb  \\\n",
       "0  [-0.319358766078949, 0.01164466142654419, -0.1...   \n",
       "1  [-0.26809656620025635, 0.20610269904136658, -0...   \n",
       "2  [-0.15259815752506256, 0.19541406631469727, 0....   \n",
       "3  [-0.0863858014345169, 0.19652682542800903, -0....   \n",
       "4  [-0.5515077710151672, -0.2693435549736023, -0....   \n",
       "\n",
       "                                      gcn_bert_large  \n",
       "0  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "1  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "2  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "3  [0.009847326, -0.00019112365, -0.0008584456, 0...  \n",
       "4  [0.009847326, -0.00019112365, -0.0008584456, 0...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       0\n",
       "box                      0\n",
       "category                 0\n",
       "text                     0\n",
       "relations                0\n",
       "gap                      0\n",
       "text_density             0\n",
       "text_number              0\n",
       "char_density             0\n",
       "char_number              0\n",
       "parsing_level1       12486\n",
       "parsing_level2       12486\n",
       "near_gap                 0\n",
       "gcn_token_number         0\n",
       "visual_embeddings        0\n",
       "bert_large_emb           0\n",
       "gcn_bert_large           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     int64\n",
       "box                   object\n",
       "category              object\n",
       "text                  object\n",
       "relations             object\n",
       "gap                   object\n",
       "text_density         float64\n",
       "text_number            int64\n",
       "char_density         float64\n",
       "char_number            int64\n",
       "parsing_level1        object\n",
       "parsing_level2        object\n",
       "near_gap              object\n",
       "gcn_token_number      object\n",
       "visual_embeddings     object\n",
       "bert_large_emb        object\n",
       "gcn_bert_large        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                   0\n",
       "box                                               [237, 211, 301, 237]\n",
       "category                                                         title\n",
       "text                                                             Value\n",
       "relations            {'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...\n",
       "gap                  {'1': 7, '2': 38, '3': 178, '4': 207, '5': 386...\n",
       "text_density                                                  0.000601\n",
       "text_number                                                          1\n",
       "char_density                                                  0.003005\n",
       "char_number                                                          5\n",
       "parsing_level1                                                      []\n",
       "parsing_level2                                                      []\n",
       "near_gap                                           {'39': 3, '188': 4}\n",
       "gcn_token_number     [0.028947916, -0.05766789, -0.024570161, 0.132...\n",
       "visual_embeddings    [0.0, 1.6673742532730103, 2.7821877002716064, ...\n",
       "bert_large_emb       [-0.319358766078949, 0.01164466142654419, -0.1...\n",
       "gcn_bert_large       [0.009847326, -0.00019112365, -0.0008584456, 0...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['gcn_token_number'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['visual_embeddings'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['bert_large_emb'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['gcn_bert_large'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train['bert_large_emb'] = np.asarray(df_train['bert_large_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test = pd.merge(df_dw_test, df_pc_test[['text','bert_large_emb','gcn_bert_large']], on='text', how='inner') \n",
    "\n",
    "\n",
    "df_test = pd.merge(df_dw_test, df_pc_test[['id','bert_large_emb','gcn_bert_large']]\\\n",
    "                    , on=['id','id'], how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10508, 15)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>box</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>relations</th>\n",
       "      <th>gap</th>\n",
       "      <th>text_density</th>\n",
       "      <th>text_number</th>\n",
       "      <th>char_density</th>\n",
       "      <th>char_number</th>\n",
       "      <th>near_gap</th>\n",
       "      <th>gcn_token_number</th>\n",
       "      <th>visual_embeddings</th>\n",
       "      <th>bert_large_emb</th>\n",
       "      <th>gcn_bert_large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[147, 95, 160, 105]</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>20</td>\n",
       "      <td>{'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...</td>\n",
       "      <td>{'1': 128, '2': 147, '3': 204, '4': 225, '5': ...</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>2</td>\n",
       "      <td>{'160': 5, '191': 4}</td>\n",
       "      <td>[0.031871974, -0.059624366, -0.02175792, 0.132...</td>\n",
       "      <td>[0.0, 0.0007124908152036369, 0.044703572988510...</td>\n",
       "      <td>[-0.17854946851730347, 0.14459457993507385, -0...</td>\n",
       "      <td>[0.010025085, -0.00017643969, -0.00085669063, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[288, 95, 302, 105]</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>N.</td>\n",
       "      <td>{'0': {'id': 506, 'object': 0}, '1': {'id': 50...</td>\n",
       "      <td>{'0': 128, '2': 5, '3': 62, '4': 83, '5': 161,...</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>2</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>2</td>\n",
       "      <td>{'45': 3, '102': 2}</td>\n",
       "      <td>[0.0006831532, -0.061630193, -0.026863059, 0.0...</td>\n",
       "      <td>[0.0, 0.4288972020149231, 2.782357931137085, 0...</td>\n",
       "      <td>[-0.47615328431129456, 0.031131230294704437, 0...</td>\n",
       "      <td>[0.010025085, -0.00017643969, -0.00085669063, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[307, 95, 360, 105]</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>KRAUS,</td>\n",
       "      <td>{'0': {'id': 1012, 'object': 0}, '1': {'id': 1...</td>\n",
       "      <td>{'0': 147, '1': 5, '3': 4, '4': 25, '5': 103, ...</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>6</td>\n",
       "      <td>{'3': 4, '173': 4}</td>\n",
       "      <td>[-0.0013003651, -0.0588799, -0.02303212, 0.078...</td>\n",
       "      <td>[0.0, 0.9150639772415161, 2.0943284034729004, ...</td>\n",
       "      <td>[-0.15259815752506256, 0.19541412591934204, 0....</td>\n",
       "      <td>[0.010025085, -0.00017643969, -0.00085669063, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[364, 95, 381, 105]</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>M.</td>\n",
       "      <td>{'0': {'id': 1518, 'object': 0}, '1': {'id': 1...</td>\n",
       "      <td>{'0': 204, '1': 62, '2': 4, '4': 4, '5': 82, '...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>2</td>\n",
       "      <td>{'392': 1, '397': 1, '407': 1}</td>\n",
       "      <td>[-0.00032287612, -0.063181095, -0.024639502, 0...</td>\n",
       "      <td>[0.0, 0.492950975894928, 1.6979507207870483, 0...</td>\n",
       "      <td>[-0.17854946851730347, 0.14459457993507385, -0...</td>\n",
       "      <td>[0.010025085, -0.00017643969, -0.00085669063, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[385, 93, 458, 105]</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>ESCARDO,</td>\n",
       "      <td>{'0': {'id': 2024, 'object': 0}, '1': {'id': 2...</td>\n",
       "      <td>{'0': 225, '1': 83, '2': 25, '3': 4, '5': 5, '...</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>2</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>9</td>\n",
       "      <td>{'217': 1, '252': 1, '318': 1, '469': 1}</td>\n",
       "      <td>[-0.0008390443, -0.0633389, -0.021429792, 0.07...</td>\n",
       "      <td>[0.0, 0.8936205506324768, 2.011612892150879, 0...</td>\n",
       "      <td>[-0.4166693389415741, 0.4152009189128876, -0.5...</td>\n",
       "      <td>[0.010025085, -0.00017643969, -0.00085669063, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                  box   category       text  \\\n",
       "0   0  [147, 95, 160, 105]  paragraph         20   \n",
       "1   1  [288, 95, 302, 105]  paragraph         N.   \n",
       "2   2  [307, 95, 360, 105]  paragraph     KRAUS,   \n",
       "3   3  [364, 95, 381, 105]  paragraph         M.   \n",
       "4   4  [385, 93, 458, 105]  paragraph  ESCARDO,   \n",
       "\n",
       "                                           relations  \\\n",
       "0  {'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...   \n",
       "1  {'0': {'id': 506, 'object': 0}, '1': {'id': 50...   \n",
       "2  {'0': {'id': 1012, 'object': 0}, '1': {'id': 1...   \n",
       "3  {'0': {'id': 1518, 'object': 0}, '1': {'id': 1...   \n",
       "4  {'0': {'id': 2024, 'object': 0}, '1': {'id': 2...   \n",
       "\n",
       "                                                 gap  text_density  \\\n",
       "0  {'1': 128, '2': 147, '3': 204, '4': 225, '5': ...      0.007692   \n",
       "1  {'0': 128, '2': 5, '3': 62, '4': 83, '5': 161,...      0.014286   \n",
       "2  {'0': 147, '1': 5, '3': 4, '4': 25, '5': 103, ...      0.003774   \n",
       "3  {'0': 204, '1': 62, '2': 4, '4': 4, '5': 82, '...      0.011765   \n",
       "4  {'0': 225, '1': 83, '2': 25, '3': 4, '5': 5, '...      0.002283   \n",
       "\n",
       "   text_number  char_density  char_number  \\\n",
       "0            1      0.015385            2   \n",
       "1            2      0.014286            2   \n",
       "2            2      0.011321            6   \n",
       "3            2      0.011765            2   \n",
       "4            2      0.010274            9   \n",
       "\n",
       "                                   near_gap  \\\n",
       "0                      {'160': 5, '191': 4}   \n",
       "1                       {'45': 3, '102': 2}   \n",
       "2                        {'3': 4, '173': 4}   \n",
       "3            {'392': 1, '397': 1, '407': 1}   \n",
       "4  {'217': 1, '252': 1, '318': 1, '469': 1}   \n",
       "\n",
       "                                    gcn_token_number  \\\n",
       "0  [0.031871974, -0.059624366, -0.02175792, 0.132...   \n",
       "1  [0.0006831532, -0.061630193, -0.026863059, 0.0...   \n",
       "2  [-0.0013003651, -0.0588799, -0.02303212, 0.078...   \n",
       "3  [-0.00032287612, -0.063181095, -0.024639502, 0...   \n",
       "4  [-0.0008390443, -0.0633389, -0.021429792, 0.07...   \n",
       "\n",
       "                                   visual_embeddings  \\\n",
       "0  [0.0, 0.0007124908152036369, 0.044703572988510...   \n",
       "1  [0.0, 0.4288972020149231, 2.782357931137085, 0...   \n",
       "2  [0.0, 0.9150639772415161, 2.0943284034729004, ...   \n",
       "3  [0.0, 0.492950975894928, 1.6979507207870483, 0...   \n",
       "4  [0.0, 0.8936205506324768, 2.011612892150879, 0...   \n",
       "\n",
       "                                      bert_large_emb  \\\n",
       "0  [-0.17854946851730347, 0.14459457993507385, -0...   \n",
       "1  [-0.47615328431129456, 0.031131230294704437, 0...   \n",
       "2  [-0.15259815752506256, 0.19541412591934204, 0....   \n",
       "3  [-0.17854946851730347, 0.14459457993507385, -0...   \n",
       "4  [-0.4166693389415741, 0.4152009189128876, -0.5...   \n",
       "\n",
       "                                      gcn_bert_large  \n",
       "0  [0.010025085, -0.00017643969, -0.00085669063, ...  \n",
       "1  [0.010025085, -0.00017643969, -0.00085669063, ...  \n",
       "2  [0.010025085, -0.00017643969, -0.00085669063, ...  \n",
       "3  [0.010025085, -0.00017643969, -0.00085669063, ...  \n",
       "4  [0.010025085, -0.00017643969, -0.00085669063, ...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density',\n",
       "       'text_number', 'char_density', 'char_number', 'parsing_level1',\n",
       "       'parsing_level2', 'near_gap', 'gcn_token_number', 'visual_embeddings',\n",
       "       'bert_large_emb', 'gcn_bert_large'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density',\n",
       "       'text_number', 'char_density', 'char_number', 'near_gap',\n",
       "       'gcn_token_number', 'visual_embeddings', 'bert_large_emb',\n",
       "       'gcn_bert_large'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Added extra code here - DO NOT RUN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "df_train['label'] = labelencoder.fit_transform(df_train['category'])\n",
    "df_test['label'] = labelencoder.fit_transform(df_test['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     9334\n",
       "2      708\n",
       "10     603\n",
       "6      594\n",
       "4      501\n",
       "8      469\n",
       "0      399\n",
       "1      118\n",
       "9       81\n",
       "11      36\n",
       "5       18\n",
       "3        6\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     7965\n",
       "4     1136\n",
       "8      581\n",
       "0      508\n",
       "2      137\n",
       "1       55\n",
       "9       50\n",
       "10      43\n",
       "6       18\n",
       "3        9\n",
       "5        6\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'author', 'caption', 'date', 'equation', 'figure', 'footer', 'paragraph', 'reference', 'section', 'title'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_mapping = {l: i for i, l in enumerate(labelencoder.classes_)}\n",
    "integer_mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# new_df = pd.DataFrame()\n",
    "# new_df_test = pd.DataFrame()\n",
    "\n",
    "# cols_left  = ['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
    "#                    'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
    "#                    'gcn_pos_emb','visual_embedding','gcn_bert_large']\n",
    "# cols_right = ['text', 'label', 'text_density', 'char_density',\\\n",
    "#                     'char_number', 'parsing_level1', 'parsing_level2',\\\n",
    "#                     'gcn_token_number', 'visual_embeddings', 'bert_large_emb']\n",
    "\n",
    "\n",
    "# new_df[cols_left] = df_train[cols_right]\n",
    "# new_df_test[cols_left] = df_test[cols_right]\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['text', \n",
    " 'label',\n",
    " 'near_visual_feature',\t\n",
    " 'gcn_near_char_density',\t\n",
    " 'gcn_near_char_number',              \n",
    " 'level1_parse_emb',\n",
    " 'level2_parse_emb',\n",
    " 'gcn_near_token_density',\n",
    " 'density',\n",
    " 'visual_feature',\n",
    " 'gcn_bert_predicted']\n",
    "\n",
    "\n",
    "Index(['id', 'box', 'category', 'text', 'relations', 'gap', 'text_density',\n",
    "       'text_number', 'char_density', 'char_number', 'parsing_level1',\n",
    "       'parsing_level2', 'near_gap', 'gcn_token_number', 'visual_embeddings',\n",
    "       'bert_large_emb', 'gcn_bert_large'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test['parsing_level1'] = df_test.apply(lambda x: [], axis=1)\n",
    "df_test['parsing_level2'] = df_test.apply(lambda x: [], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.DataFrame()\n",
    "new_df_test = pd.DataFrame()\n",
    "\n",
    "cols_left  = ['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
    "                   'gcn_near_char_number','level1_parse_emb','level2_parse_emb',\\\n",
    "                   'density','visual_feature','gcn_bert_predicted']\n",
    "\n",
    "# cols_right = ['text', 'label', 'gcn_token_number', 'char_density',\\\n",
    "#                     'char_number', 'parsing_level1', 'parsing_level2',\\\n",
    "#                     'text_density', 'visual_embeddings', 'bert_large_emb']\n",
    "\n",
    "cols_right = ['text', 'label', 'gcn_token_number', 'char_density',\\\n",
    "                    'char_number', 'parsing_level1', 'parsing_level2',\\\n",
    "                    'text_density', 'visual_embeddings', 'gcn_bert_large']\n",
    "\n",
    "\n",
    "new_df[cols_left] = df_train[cols_right]\n",
    "new_df_test[cols_left] = df_test[cols_right]\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12867, 18)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../docbank_data/df_train_docgcn_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10508, 18)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_csv(\"../docbank_data/df_test_docgcn_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train[\"gcn_token_number\"].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train[\"gcn_token_number\"].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>box</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>relations</th>\n",
       "      <th>gap</th>\n",
       "      <th>text_density</th>\n",
       "      <th>text_number</th>\n",
       "      <th>char_density</th>\n",
       "      <th>char_number</th>\n",
       "      <th>parsing_level1</th>\n",
       "      <th>parsing_level2</th>\n",
       "      <th>near_gap</th>\n",
       "      <th>gcn_token_number</th>\n",
       "      <th>visual_embeddings</th>\n",
       "      <th>bert_large_emb</th>\n",
       "      <th>gcn_bert_large</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[237, 211, 301, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>Value</td>\n",
       "      <td>{'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...</td>\n",
       "      <td>{'1': 7, '2': 38, '3': 178, '4': 207, '5': 386...</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'39': 3, '188': 4}</td>\n",
       "      <td>[0.028947916, -0.05766789, -0.024570161, 0.132...</td>\n",
       "      <td>[0.0, 1.6673742532730103, 2.7821877002716064, ...</td>\n",
       "      <td>[-0.319358766078949, 0.01164466142654419, -0.1...</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[308, 211, 332, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>of</td>\n",
       "      <td>{'0': {'id': 243, 'object': 0}, '1': {'id': 24...</td>\n",
       "      <td>{'0': 7, '2': 7, '3': 147, '4': 176, '5': 355,...</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'138': 1, '176': 1}</td>\n",
       "      <td>[0.03022929, -0.058581498, -0.02262115, 0.1348...</td>\n",
       "      <td>[0.0, 0.9130946397781372, 0.8139663934707642, ...</td>\n",
       "      <td>[-0.26809656620025635, 0.20610269904136658, -0...</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[339, 211, 472, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>information</td>\n",
       "      <td>{'0': {'id': 486, 'object': 0}, '1': {'id': 48...</td>\n",
       "      <td>{'0': 38, '1': 7, '3': 7, '4': 36, '5': 215, '...</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'76': 4, '213': 5}</td>\n",
       "      <td>[0.029282099, -0.061983045, -0.022321349, 0.12...</td>\n",
       "      <td>[0.0, 1.0808653831481934, 1.1382662057876587, ...</td>\n",
       "      <td>[-0.15259815752506256, 0.19541406631469727, 0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[479, 211, 501, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>in</td>\n",
       "      <td>{'0': {'id': 729, 'object': 0}, '1': {'id': 73...</td>\n",
       "      <td>{'0': 178, '1': 147, '2': 7, '4': 7, '5': 186,...</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'130': 3, '181': 6}</td>\n",
       "      <td>[0.029444845, -0.0605363, -0.027396373, 0.1415...</td>\n",
       "      <td>[0.0, 0.7543953657150269, 2.0874814987182617, ...</td>\n",
       "      <td>[-0.0863858014345169, 0.19652682542800903, -0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[508, 211, 680, 237]</td>\n",
       "      <td>title</td>\n",
       "      <td>noncooperative</td>\n",
       "      <td>{'0': {'id': 972, 'object': 0}, '1': {'id': 97...</td>\n",
       "      <td>{'0': 207, '1': 176, '2': 36, '3': 7, '5': 7, ...</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>14</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'92': 1, '136': 1, '226': 1}</td>\n",
       "      <td>[0.03045706, -0.060177967, -0.030296788, 0.146...</td>\n",
       "      <td>[0.0, 1.8353612422943115, 0.5946379899978638, ...</td>\n",
       "      <td>[-0.5515077710151672, -0.2693435549736023, -0....</td>\n",
       "      <td>[0.009847326, -0.00019112365, -0.0008584456, 0...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                   box category            text  \\\n",
       "0   0  [237, 211, 301, 237]    title           Value   \n",
       "1   1  [308, 211, 332, 237]    title              of   \n",
       "2   2  [339, 211, 472, 237]    title     information   \n",
       "3   3  [479, 211, 501, 237]    title              in   \n",
       "4   4  [508, 211, 680, 237]    title  noncooperative   \n",
       "\n",
       "                                           relations  \\\n",
       "0  {'0': {'id': 0, 'object': 1}, '1': {'id': 1, '...   \n",
       "1  {'0': {'id': 243, 'object': 0}, '1': {'id': 24...   \n",
       "2  {'0': {'id': 486, 'object': 0}, '1': {'id': 48...   \n",
       "3  {'0': {'id': 729, 'object': 0}, '1': {'id': 73...   \n",
       "4  {'0': {'id': 972, 'object': 0}, '1': {'id': 97...   \n",
       "\n",
       "                                                 gap  text_density  \\\n",
       "0  {'1': 7, '2': 38, '3': 178, '4': 207, '5': 386...      0.000601   \n",
       "1  {'0': 7, '2': 7, '3': 147, '4': 176, '5': 355,...      0.001603   \n",
       "2  {'0': 38, '1': 7, '3': 7, '4': 36, '5': 215, '...      0.000289   \n",
       "3  {'0': 178, '1': 147, '2': 7, '4': 7, '5': 186,...      0.001748   \n",
       "4  {'0': 207, '1': 176, '2': 36, '3': 7, '5': 7, ...      0.000224   \n",
       "\n",
       "   text_number  char_density  char_number parsing_level1 parsing_level2  \\\n",
       "0            1      0.003005            5             []             []   \n",
       "1            1      0.003205            2             []             []   \n",
       "2            1      0.003181           11             []             []   \n",
       "3            1      0.003497            2             []             []   \n",
       "4            1      0.003131           14             []             []   \n",
       "\n",
       "                        near_gap  \\\n",
       "0            {'39': 3, '188': 4}   \n",
       "1           {'138': 1, '176': 1}   \n",
       "2            {'76': 4, '213': 5}   \n",
       "3           {'130': 3, '181': 6}   \n",
       "4  {'92': 1, '136': 1, '226': 1}   \n",
       "\n",
       "                                    gcn_token_number  \\\n",
       "0  [0.028947916, -0.05766789, -0.024570161, 0.132...   \n",
       "1  [0.03022929, -0.058581498, -0.02262115, 0.1348...   \n",
       "2  [0.029282099, -0.061983045, -0.022321349, 0.12...   \n",
       "3  [0.029444845, -0.0605363, -0.027396373, 0.1415...   \n",
       "4  [0.03045706, -0.060177967, -0.030296788, 0.146...   \n",
       "\n",
       "                                   visual_embeddings  \\\n",
       "0  [0.0, 1.6673742532730103, 2.7821877002716064, ...   \n",
       "1  [0.0, 0.9130946397781372, 0.8139663934707642, ...   \n",
       "2  [0.0, 1.0808653831481934, 1.1382662057876587, ...   \n",
       "3  [0.0, 0.7543953657150269, 2.0874814987182617, ...   \n",
       "4  [0.0, 1.8353612422943115, 0.5946379899978638, ...   \n",
       "\n",
       "                                      bert_large_emb  \\\n",
       "0  [-0.319358766078949, 0.01164466142654419, -0.1...   \n",
       "1  [-0.26809656620025635, 0.20610269904136658, -0...   \n",
       "2  [-0.15259815752506256, 0.19541406631469727, 0....   \n",
       "3  [-0.0863858014345169, 0.19652682542800903, -0....   \n",
       "4  [-0.5515077710151672, -0.2693435549736023, -0....   \n",
       "\n",
       "                                      gcn_bert_large  label  \n",
       "0  [0.009847326, -0.00019112365, -0.0008584456, 0...     11  \n",
       "1  [0.009847326, -0.00019112365, -0.0008584456, 0...     11  \n",
       "2  [0.009847326, -0.00019112365, -0.0008584456, 0...     11  \n",
       "3  [0.009847326, -0.00019112365, -0.0008584456, 0...     11  \n",
       "4  [0.009847326, -0.00019112365, -0.0008584456, 0...     11  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "baSmeDdIEadM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df = df_train[['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
    "#                    'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
    "#                    'gcn_bert_base','gcn_pos_emb','visual_embedding','gcn_bert_large']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "cLrlbfWr-VE_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "RXgrBoaeQIaX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "id": "qX1g2eqC0s4I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df_test = df_test[['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
    "#                        'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
    "#                        'gcn_bert_base','gcn_pos_emb','visual_embedding','gcn_bert_large']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "tKXr_-Fh-Y5z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(new_df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbiQjAgdSqhn",
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../docbank_data/df_train_docgcn_final.csv\")\n",
    "df_test = pd.read_csv(\"../docbank_data/df_test_docgcn_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "nvXxpfNCGER2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 2e-05\n",
    "# Change the pre-trained bert model\n",
    "#tokenizer = BertTokenizer.from_pretrained('roberta-base') #Cased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "3vWRDemOGxJD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.label\n",
    "        self.gcn_visual_feature = dataframe.near_visual_feature\n",
    "        self.visual_feature = dataframe.visual_feature\n",
    "        self.gcn_bert_base = dataframe.gcn_bert_predicted\n",
    "        self.parsing1 = dataframe.level1_parse_emb\n",
    "        self.parsing2 = dataframe.level2_parse_emb\n",
    "        self.char_density = dataframe.gcn_near_char_density\n",
    "        self.char_number = dataframe.gcn_near_char_number\n",
    "        self.density = dataframe.density\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
    "            'density': torch.tensor(self.density[index],dtype=torch.float),\n",
    "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
    "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
    "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
    "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
    "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
    "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
    "            'gcn_visual_feature': torch.tensor(self.gcn_visual_feature[index],dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "7Gpe9D1QHoCd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (12867, 10)\n",
      "TRAIN Dataset: (12867, 10)\n",
      "TEST Dataset: (10508, 10)\n",
      "NEW TRAIN Dataset: (10000, 10)\n",
      "NEW EVAL Dataset: (2867, 10)\n"
     ]
    }
   ],
   "source": [
    "train_size = 1\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "#test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "new_train_data = train_data[:10000]\n",
    "new_eval_data = train_data[10000:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# train, val = train.reset_index(drop=True), val.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(new_df_test.shape))\n",
    "\n",
    "print(\"NEW TRAIN Dataset: {}\".format(new_train_data.shape))\n",
    "print(\"NEW EVAL Dataset: {}\".format(new_eval_data.shape))\n",
    "\n",
    "\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "new_training_set = SentimentData(new_train_data, tokenizer, MAX_LEN)\n",
    "new_eval_set = SentimentData(new_eval_data, tokenizer, MAX_LEN)\n",
    "#testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
    "test_set = SentimentData(new_df_test,tokenizer,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    zipped = zip(data)\n",
    "    return list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "c1tInLk2Eadt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "eval_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "new_training_loader = DataLoader(new_training_set, **train_params)\n",
    "#testing_loader = DataLoader(testing_set, **test_params)\n",
    "vali_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "eval_loader = DataLoader(new_eval_set, **eval_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF12YgfxSwEr",
    "tags": []
   },
   "source": [
    "## Define the proposed classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipKk-m3pynW6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "HMqQTafXEaei",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class RobertaClass(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(RobertaClass, self).__init__()\n",
    "#         #bert-base-cased 768\n",
    "#         #bert-large-cased bert-large-uncased 1024\n",
    "#         #roberta-base-cased 768\n",
    "#         #biobert\n",
    "\n",
    "#         self.l1 = AutoModel.from_pretrained('bert-base-uncased')# BERT large\n",
    "#         self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "#         self.dropout = torch.nn.Dropout(0.1)\n",
    "#         self.h1 = torch.nn.Linear(768,768)\n",
    "#         self.h2 = torch.nn.Linear(768,768)\n",
    "#         self.h3 = torch.nn.Linear(768,768)\n",
    "#         self.h4 = torch.nn.Linear(768,768)\n",
    "#         self.hidden0 = torch.nn.Linear(2304,256)\n",
    "#         self.hidden1 = torch.nn.Linear(2048,768)\n",
    "#         self.hidden2 = torch.nn.Linear(3072,512)\n",
    "\n",
    "#         self.hidden3 = torch.nn.Linear(3072,3072)\n",
    "#         self.hidden4 = torch.nn.Linear(1024,768)\n",
    "#         self.classifier = torch.nn.Linear(512, 4)\n",
    "#         self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
    "\n",
    "#     def forward(self,input_ids,attention_mask,token_type_ids,char_density,char_number, visual_feature,\\\n",
    "#                 gcn_bert_base,parsing1,parsing2,pos_emb,visual):\n",
    "\n",
    "#         output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "#         hidden_state = output_1[0]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         # BERT 768 BERT / large 1024\n",
    "        \n",
    "#         # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
    "#         pooler = self.pre_classifier(pooler)\n",
    "#         pooler = torch.nn.Tanh()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "\n",
    "#         other = torch.cat((pooler.unsqueeze(1),gcn_bert_base.unsqueeze(1)),1)\n",
    "#         other = self.pooling(other).squeeze(1)\n",
    "#         other = self.h1(other)\n",
    "#         other = torch.nn.Tanh()(other)\n",
    "#         other = self.dropout(other)\n",
    "#         pooler = other\n",
    "\n",
    "\n",
    "#         parsing = torch.cat((parsing1.unsqueeze(1),parsing2.unsqueeze(1)),1)\n",
    "#         parsing = self.pooling(parsing).squeeze(1)\n",
    "#         parsing = self.h2(parsing)\n",
    "#         parsing = torch.nn.Tanh()(parsing)\n",
    "#         parsing = self.dropout(parsing)\n",
    "\n",
    "#         density = torch.cat((char_density.unsqueeze(1),char_number.unsqueeze(1)),1)\n",
    "#         density = self.pooling(density).squeeze(1)\n",
    "#         density = self.h3(density)\n",
    "#         density = torch.nn.Tanh()(density)\n",
    "#         density = self.dropout(density)\n",
    "\n",
    "#         visual = self.hidden1(visual)\n",
    "#         visual = torch.nn.Tanh()(visual)\n",
    "#         visual = self.dropout(visual)\n",
    "\n",
    "#         other = torch.cat((visual.unsqueeze(1),visual_feature.unsqueeze(1)),1)\n",
    "#         other = self.pooling(other).squeeze(1)\n",
    "#         other = self.h4(other)\n",
    "#         other = torch.nn.Tanh()(other)\n",
    "#         other = self.dropout(other)\n",
    "#         visual = other\n",
    "\n",
    "#         pooler = torch.cat((pooler,visual,parsing,char_density),1)\n",
    "#         pooler = self.hidden3(pooler)\n",
    "#         pooler = torch.nn.Tanh()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "\n",
    "#         pooler = self.hidden2(pooler)\n",
    "#         pooler = torch.nn.Tanh()(pooler)\n",
    "#         pooler = self.dropout(pooler)\n",
    "\n",
    "#         output = self.classifier(pooler)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with 1024 DIm\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        #bert-base-cased 768\n",
    "        #bert-large-cased bert-large-uncased 1024\n",
    "        #roberta-base-cased 768\n",
    "        #biobert\n",
    "\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\")# BERT large\n",
    "        self.pre_classifier = torch.nn.Linear(768, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.hidden_cls = torch.nn.Linear(1024,768)\n",
    "        self.hidden_parsing = torch.nn.Linear(768,768)\n",
    "        self.hidden_den = torch.nn.Linear(768,768)\n",
    "        self.hidden_vis = torch.nn.Linear(768,768)\n",
    "        self.hidden_vis_pro = torch.nn.Linear(2048,768)\n",
    "        self.hidden_all = torch.nn.Linear(768*2,768*2)\n",
    "        self.before_classifier = torch.nn.Linear(768*2,128)\n",
    "        self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
    "        self.classifier = torch.nn.Linear(128, 13)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, char_density,char_number,visual_feature,bert_cls,parsing1,parsing2,visual):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "\n",
    "        # BERT 768 BERT / large 1024\n",
    "        \n",
    "        # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
    "        \n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        pooler = torch.cat((pooler.unsqueeze(1),bert_cls.unsqueeze(1)),1)\n",
    "        pooler = self.pooling(pooler).squeeze(1)\n",
    "        pooler = self.hidden_cls(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        visual = self.hidden_vis_pro(visual)\n",
    "        visual = torch.nn.Tanh()(visual)\n",
    "        visual = self.dropout(visual)\n",
    "\n",
    "        visual = torch.cat((visual.unsqueeze(1),visual_feature.unsqueeze(1)),1)\n",
    "        visual = self.pooling(visual).squeeze(1)\n",
    "        visual = self.hidden_vis(visual)\n",
    "        visual = torch.nn.Tanh()(visual)\n",
    "        visual = self.dropout(visual)\n",
    "\n",
    "        pooler = torch.cat((pooler,visual),1)\n",
    "        pooler = self.hidden_all(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        pooler = self.before_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with 768 dim\n",
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        #bert-base-cased 768\n",
    "        #bert-large-cased bert-large-uncased 1024\n",
    "        #roberta-base-cased 768\n",
    "        #biobert\n",
    "\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\")# BERT large\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.hidden_cls = torch.nn.Linear(768,768)\n",
    "        self.hidden_parsing = torch.nn.Linear(768,768)\n",
    "        self.hidden_den = torch.nn.Linear(768,768)\n",
    "        self.hidden_vis = torch.nn.Linear(768,768)\n",
    "        self.hidden_vis_pro = torch.nn.Linear(2048,768)\n",
    "        self.hidden_all = torch.nn.Linear(768*2,768*2)\n",
    "        self.before_classifier = torch.nn.Linear(768*2,128)\n",
    "        self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
    "        self.classifier = torch.nn.Linear(128, 13)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, char_density,char_number,visual_feature,bert_cls,parsing1,parsing2,visual):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "\n",
    "        # BERT 768 BERT / large 1024\n",
    "        \n",
    "        # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
    "        \n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        pooler = torch.cat((pooler.unsqueeze(1),bert_cls.unsqueeze(1)),1)\n",
    "        pooler = self.pooling(pooler).squeeze(1)\n",
    "        pooler = self.hidden_cls(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        visual = self.hidden_vis_pro(visual)\n",
    "        visual = torch.nn.Tanh()(visual)\n",
    "        visual = self.dropout(visual)\n",
    "\n",
    "        visual = torch.cat((visual.unsqueeze(1),visual_feature.unsqueeze(1)),1)\n",
    "        visual = self.pooling(visual).squeeze(1)\n",
    "        visual = self.hidden_vis(visual)\n",
    "        visual = torch.nn.Tanh()(visual)\n",
    "        visual = self.dropout(visual)\n",
    "\n",
    "        pooler = torch.cat((pooler,visual),1)\n",
    "        pooler = self.hidden_all(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        pooler = self.before_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "id": "sZ55mIPZIkp_",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (hidden_cls): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  (hidden_parsing): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_den): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_vis): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_vis_pro): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  (hidden_all): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "  (before_classifier): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = RobertaClass()\n",
    "new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XBJjGKdS2b8",
    "tags": []
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "XYZ7YuJ5InOS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  new_model.parameters(), lr=1e-05) # change learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "yPhA2V3iIpzN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:00:09.490335: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default logging directory  `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('docbank_runs/docbank_docgcn_experiment_1')\n",
    "new_writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 22:20:26.662706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "new_writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m7\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "next(dataiter)['targets'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2137: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(new_training_loader)\n",
    "labels = next(dataiter)['targets']\n",
    "masks = next(dataiter)['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[173], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[  101,  3196,  9402,  ...,     0,     0,     0],\n",
       "         [  101, 27178,  2102,  ...,     0,     0,     0],\n",
       "         [  101,  3058,   102,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  5246,   102,  ...,     0,     0,     0],\n",
       "         [  101,  6583,   102,  ...,     0,     0,     0],\n",
       "         [  101,   100,   102,  ...,     0,     0,     0]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'targets': tensor([0., 1., 3., 3., 3., 0., 0., 3., 2., 3., 3., 3., 0., 3., 0., 0.]),\n",
       " 'density': tensor([0.0020, 0.0017, 0.0010, 0.0011, 0.0038, 0.0032, 0.0008, 0.0036, 0.0011,\n",
       "         0.0022, 0.0022, 0.0011, 0.0125, 0.0046, 0.0077, 0.0024]),\n",
       " 'gcn_bert_base': tensor([[-0.5740,  0.1609,  0.0450,  ..., -0.1054,  0.7080,  0.6565],\n",
       "         [-0.1194,  0.4650, -0.1720,  ..., -0.1511,  0.7631,  0.3729],\n",
       "         [-0.6775, -0.5910, -0.0881,  ..., -0.5313,  0.9908,  0.0846],\n",
       "         ...,\n",
       "         [-0.5863, -0.0132,  0.4435,  ..., -0.2423,  0.4817,  0.5287],\n",
       "         [-0.1893,  0.0702, -0.1817,  ..., -0.4111,  0.3171,  0.6708],\n",
       "         [-0.1893,  0.0702, -0.1817,  ..., -0.4111,  0.3171,  0.6708]]),\n",
       " 'char_density': tensor([0.0101, 0.0113, 0.0041, 0.0044, 0.0229, 0.0097, 0.0068, 0.0214, 0.0050,\n",
       "         0.0073, 0.0089, 0.0064, 0.0125, 0.0231, 0.0154, 0.0024]),\n",
       " 'char_number': tensor([10., 13.,  4.,  8., 30.,  3., 26., 12., 35., 13., 12., 12.,  1.,  5.,\n",
       "          2.,  1.]),\n",
       " 'visual_feature': tensor([[0.0000, 1.6584, 5.7633,  ..., 0.0000, 0.6913, 0.0000],\n",
       "         [0.0000, 0.8001, 3.2654,  ..., 0.0000, 1.4020, 0.0000],\n",
       "         [0.0000, 3.7423, 2.7210,  ..., 0.0000, 0.0000, 0.5018],\n",
       "         ...,\n",
       "         [0.0000, 0.0176, 7.8255,  ..., 0.0000, 0.0270, 0.0000],\n",
       "         [0.0000, 0.8650, 3.5739,  ..., 0.0000, 0.0374, 0.0000],\n",
       "         [0.0000, 0.3113, 3.4536,  ..., 0.0000, 0.0000, 0.4239]]),\n",
       " 'parsing1': tensor([], size=(16, 0)),\n",
       " 'parsing2': tensor([], size=(16, 0)),\n",
       " 'gcn_visual_feature': tensor([[-0.0316, -0.0438, -0.1805,  ..., -0.0594, -0.0242, -0.0629],\n",
       "         [-0.0322, -0.0484, -0.1702,  ..., -0.0657, -0.0350, -0.1457],\n",
       "         [-0.0264, -0.0374, -0.1796,  ..., -0.0590, -0.0335, -0.0201],\n",
       "         ...,\n",
       "         [-0.0273, -0.0430, -0.1931,  ..., -0.0612, -0.0270,  0.1408],\n",
       "         [-0.0294, -0.0378, -0.2038,  ..., -0.0555, -0.0366,  0.1356],\n",
       "         [-0.0292, -0.0395, -0.1784,  ..., -0.0548, -0.0307,  0.1235]])}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 7., 7., 7., 6., 7., 6., 7., 7., 7.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 9 required positional arguments: 'attention_mask', 'token_type_ids', 'char_density', 'char_number', 'visual_feature', 'bert_cls', 'parsing1', 'parsing2', and 'visual'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    837\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m         \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# Caffe2 models do not have the 'forward' method\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m caffe2_pb2\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m         trace \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m         graph \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mgraph\n\u001b[1;32m    339\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/jit/_trace.py:794\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m ):\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/jit/_trace.py:1056\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1056\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 9 required positional arguments: 'attention_mask', 'token_type_ids', 'char_density', 'char_number', 'visual_feature', 'bert_cls', 'parsing1', 'parsing2', and 'visual'"
     ]
    }
   ],
   "source": [
    "writer.add_graph(new_model, masks)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "mhqvtY2SIup7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    output = []\n",
    "    new_model.train()\n",
    "    for _,data in tqdm(enumerate(new_training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
    "        gcn_visual_feature = data['gcn_visual_feature'].to(device, dtype = torch.float)\n",
    "        gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
    "        parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
    "        parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
    "        char_density = data['char_density'].to(device, dtype = torch.float)\n",
    "        char_number = data['char_number'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = new_model(ids, mask, token_type_ids,char_density,char_number,gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        # print(tr_loss)\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "        # print(n_correct)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        # print(_)\n",
    "        \n",
    "        \n",
    "        if _%1000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            \n",
    "            # ...log the running loss\n",
    "            new_writer.add_scalar('Mini Batch training loss',\n",
    "                            loss_step,\n",
    "                            epoch * len(new_training_loader) + _)\n",
    "            \n",
    "            new_writer.add_scalar('Mini Batch training Accuracy',\n",
    "                            accu_step,\n",
    "                            epoch * len(new_training_loader) + _)\n",
    "           \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "        # writer.flush()\n",
    "        \n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    new_writer.add_scalar(\"Epoch Train Loss\", epoch_loss, epoch)\n",
    "    new_writer.add_scalar(\"Epoch Train Accuracy\", epoch_accu, epoch)\n",
    "    \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "WHAgCduAIQRI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2096it [19:41,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m     new_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMini Batch training Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     47\u001b[0m                     accu_step,\n\u001b[1;32m     48\u001b[0m                     epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_training_loader) \u001b[38;5;241m+\u001b[39m _)\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 52\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# # When using GPU\u001b[39;00m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "  train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ecydbX_S6Lg",
    "tags": []
   },
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "bFiNcy16JLwt"
   },
   "outputs": [],
   "source": [
    "def valid(new_model, eval_loader):\n",
    "    new_model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(eval_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            char_den = data['char_density'].to(device, dtype = torch.float)\n",
    "            density = data['density'].to(device, dtype = torch.float)\n",
    "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
    "            gcn_visual_feature = data['gcn_visual_feature'].to(device, dtype = torch.float)\n",
    "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
    "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
    "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
    "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
    "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
    "            # token_density = data['token_density'].to(device, dtype = torch.float)\n",
    "            # token_number = data['token_number'].to(device, dtype = torch.float)\n",
    "\n",
    "\n",
    "            outputs = new_model(ids, mask, token_type_ids,char_density,char_number, gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            output_list = output_list + list(big_idx)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu,output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2867"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "V6Yx1waSv6iF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [13], target: [1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc,pre_list \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[209], line 25\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(new_model, eval_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# token_density = data['token_density'].to(device, dtype = torch.float)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# token_number = data['token_number'].to(device, dtype = torch.float)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m new_model(ids, mask, token_type_ids,char_density,char_number, gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m big_val, big_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [13], target: [1])"
     ]
    }
   ],
   "source": [
    "acc,pre_list = valid(new_model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "id": "CKMELmeFUaH5",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:07,  2.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc,hidden_list \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvali_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 24\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m char_number \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# token_density = data['token_density'].to(device, dtype = torch.float)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# token_number = data['token_number'].to(device, dtype = torch.float)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchar_density\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchar_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcn_visual_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgcn_bert_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparsing1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparsing2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvisual_feature\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)\n\u001b[1;32m     26\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[62], line 23\u001b[0m, in \u001b[0;36mRobertaClass.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, char_density, char_number, visual_feature, bert_cls, parsing1, parsing2, visual)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids, char_density,char_number,visual_feature,bert_cls,parsing1,parsing2,visual):\n\u001b[0;32m---> 23\u001b[0m     output_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m output_1[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m     pooler \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:958\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    949\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    951\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    952\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    953\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    957\u001b[0m )\n\u001b[0;32m--> 958\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    971\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:559\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    550\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    551\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    552\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    557\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    492\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    493\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 495\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/modeling_utils.py:1787\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 1787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:508\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    507\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 508\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 423\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    425\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# acc,hidden_list = valid(model, vali_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(new_model, '../models/docbank_docgcn_bert_layout_segment_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "LDxgkeaILFQ9"
   },
   "outputs": [],
   "source": [
    "model_to_test = torch.load('../models/docbank_docgcn_bert_layout_segment_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(new_model.state_dict(), '../models/docbank_docgcn_bert_layout_segment_classifier_v3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_8lSc-jf7bU",
    "tags": []
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "_qD3e22vE7My"
   },
   "outputs": [],
   "source": [
    "class SentimentData_test(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.max_len = max_len\n",
    "        self.visual_feature = dataframe.near_visual_feature\n",
    "        self.gcn_bert_base = dataframe.gcn_bert_predicted\n",
    "        self.parsing1 = dataframe.level1_parse_emb\n",
    "        self.parsing2 = dataframe.level2_parse_emb\n",
    "        self.char_density = dataframe.gcn_near_char_density\n",
    "        self.char_number = dataframe.gcn_near_char_number\n",
    "        # self.pos_emb = dataframe.gcn_pos_emb\n",
    "        self.visual = dataframe.visual_feature\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
    "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
    "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
    "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
    "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
    "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
    "            # 'pos_emb': torch.tensor(self.pos_emb[index],dtype=torch.float),\n",
    "            'visual': torch.tensor(self.visual[index],dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMRN5w3RXjqB",
    "tags": []
   },
   "source": [
    "### load the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_left  = ['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
    "                   'gcn_near_char_number','level1_parse_emb','level2_parse_emb',\\\n",
    "                   'density','visual_feature','gcn_bert_predicted']\n",
    "\n",
    "cols_right = ['text', 'label', 'gcn_token_number', 'char_density',\\\n",
    "                    'char_number', 'parsing_level1', 'parsing_level2',\\\n",
    "                    'text_density', 'visual_embeddings', 'bert_large_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "Zaw4D5UKFe-Z"
   },
   "outputs": [],
   "source": [
    "# new_df_true_test = new_df_test[['text', 'label','near_visual_feature','gcn_near_char_density',\\\n",
    "#                        'gcn_near_char_number','gcn_parsing1','gcn_parsing2',\\\n",
    "#                        'gcn_bert_base','gcn_pos_emb','visual_embedding','gcn_bert_large']]\n",
    "\n",
    "new_df_true_test = new_df_test[cols_left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "B0kUqjR1Fhb6"
   },
   "outputs": [],
   "source": [
    "test = SentimentData_test(new_df_true_test,tokenizer, MAX_LEN)\n",
    "testing_loader = DataLoader(test, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10508"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (hidden_cls): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_parsing): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_den): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_vis): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (hidden_vis_pro): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  (hidden_all): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "  (before_classifier): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  (pooling): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_docgcn = RobertaClass()\n",
    "model_docgcn.load_state_dict(torch.load('../models/docbank_docgcn_bert_layout_segment_classifier_v3.pt'))\n",
    "model_docgcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "IetKrn_SY-OT"
   },
   "outputs": [],
   "source": [
    "def test_label_generator(model_docgcn, testing_loader):\n",
    "    model_docgcn.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
    "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
    "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
    "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
    "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
    "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
    "            # pos_emb = data['pos_emb'].to(device, dtype = torch.float)\n",
    "            visual = data['visual'].to(device, dtype = torch.float)\n",
    "\n",
    "            # outputs = model(ids, mask, token_type_ids,visual_feature,char_density,char_number,parsing1,parsing2,gcn_bert_base,visual).squeeze()\n",
    "            outputs = model_docgcn(ids, mask, token_type_ids,char_density,char_number, visual_feature,gcn_bert_base,parsing1,parsing2,visual)\n",
    "            \n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            output_list = output_list + list(big_idx)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "qQ_noXfKBHmn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10508it [21:03,  8.32it/s]\n"
     ]
    }
   ],
   "source": [
    "output = test_label_generator(model_docgcn, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 0,\n",
       " 'author': 1,\n",
       " 'caption': 2,\n",
       " 'date': 3,\n",
       " 'equation': 4,\n",
       " 'figure': 5,\n",
       " 'footer': 6,\n",
       " 'paragraph': 7,\n",
       " 'reference': 8,\n",
       " 'section': 9,\n",
       " 'title': 10}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "id": "EtOYJuQh9dy0",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 1, 1, 1, 1, 1, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 4, 4, 4, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 4, 4, 4, 7, 7, 7, 4, 7, 7, 4, 4, 4, 4, 7, 4, 4, 7, 7, 7, 4, 4, 4, 4, 4, 4, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 4, 4, 4, 4, 4, 7, 7, 4, 4, 7, 7, 4, 4, 4, 4, 7, 4, 4, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 4, 7, 4, 7, 4, 4, 4, 7, 7, 7, 4, 4, 4, 4, 4, 7, 7, 4, 7, 7, 4, 7, 1, 1, 1, 1, 1, 1, 7, 8, 1, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 4, 4, 4, 7, 4, 4, 7, 4, 4, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 4, 4, 4, 7, 4, 4, 7, 4, 4, 7, 4, 4, 4, 7, 7, 4, 7, 4, 7, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 4, 4, 7, 4, 4, 4, 7, 4, 4, 7, 7, 7, 4, 4, 4, 4, 4, 7, 4, 7, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 0, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 4, 7, 7, 7, 7, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 4, 4, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 4, 7, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 8, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 1, 7, 7, 8, 1, 8, 8, 8, 1, 8, 1, 8, 1, 8, 8, 7, 7, 7, 7, 7, 4, 1, 7, 8, 7, 7, 7, 7, 4, 7, 7, 7, 6, 6, 0, 0, 0, 0, 0, 1, 7, 2, 2, 1, 2, 7, 2, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 7, 7, 7, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 7, 7, 7, 2, 7, 7, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 6, 0, 0, 2, 0, 0, 7, 7, 7, 8, 7, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 7, 7, 2, 7, 0, 0, 0, 6, 6, 0, 0, 7, 7, 0, 6, 6, 0, 0, 0, 7, 7, 7, 7, 0, 6, 6, 6, 0, 0, 7, 0, 7, 7, 7, 7, 6, 2, 0, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 1, 7, 7, 7, 8, 7, 7, 7, 7, 7, 6, 7, 7, 1, 7, 7, 7, 8, 2, 7, 7, 7, 7, 1, 0, 6, 1, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 0, 0, 7, 2, 0, 7, 1, 0, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 1, 1, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 0, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 1, 7, 7, 7, 7, 7, 7, 6, 6, 7, 1, 7, 7, 7, 2, 2, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 0, 0, 7, 7, 1, 1, 7, 7, 7, 7, 7, 7, 7, 0, 7, 0, 0, 0, 2, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 7, 0, 0, 0, 0, 0, 2, 1, 7, 2, 7, 7, 7, 7, 8, 7, 7, 0, 0, 7, 0, 7, 7, 7, 7, 6, 2, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 2, 0, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 7, 8, 8, 4, 8, 7, 7, 7, 7, 7, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 1, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 7, 7, 8, 8, 8, 7, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 1, 7, 7, 7, 10, 1, 1, 7, 7, 7, 7, 7, 8, 1, 1, 1, 7, 7, 4, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 1, 1, 7, 7, 7, 1, 1, 1, 1, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 1, 1, 1, 7, 4, 7, 8, 8, 4, 0, 7, 7, 7, 7, 6, 0, 6, 7, 0, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 7, 8, 7, 0, 0, 2, 0, 0, 2, 2, 2, 2, 0, 7, 7, 7, 0, 0, 0, 0, 6, 0, 2, 0, 2, 0, 0, 0, 7, 7, 7, 7, 2, 0, 0, 0, 7, 0, 7, 0, 6, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 7, 6, 2, 0, 6, 0, 1, 6, 0, 0, 6, 0, 6, 0, 0, 0, 0, 7, 7, 7, 7, 7, 0, 6, 0, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 2, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 2, 0, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 2, 2, 2, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 4, 4, 4, 4, 7, 2, 7, 2, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 2, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 7, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 8, 7, 7, 7, 7, 7, 1, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 10, 7, 7, 7, 7, 7, 7, 7, 7, 2, 0, 7, 7, 7, 7, 7, 0, 7, 7, 2, 0, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 4, 1, 7, 7, 7, 0, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 2, 2, 2, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 2, 2, 2, 7, 7, 7, 4, 7, 7, 7, 4, 7, 4, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 4, 4, 4, 4, 4, 7, 7, 2, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4, 7, 7, 4, 7, 7, 4, 7, 7, 2, 7, 7, 7, 4, 2, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 4, 4, 4, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 4, 7, 4, 4, 7, 7, 7, 7, 7, 4, 8, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 4, 4, 4, 4, 4, 7, 4, 7, 4, 4, 4, 7, 7, 2, 8, 7, 4, 4, 4, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 10, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 4, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 1, 7, 1, 1, 1, 1, 1, 7, 1, 1, 7, 7, 1, 8, 1, 7, 7, 6, 7, 0, 0, 0, 7, 7, 7, 0, 7, 0, 7, 7, 7, 0, 1, 0, 0, 0, 0, 0, 0, 7, 7, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 2, 0, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 2, 0, 7, 0, 7, 7, 0, 6, 1, 4, 8, 2, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 8, 1, 7, 2, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 0, 2, 2, 2, 2, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 2, 2, 7, 2, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 4, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 4, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 2, 7, 7, 2, 2, 7, 7, 7, 7, 4, 4, 4, 4, 4, 7, 7, 7, 4, 4, 4, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 7, 2, 7, 7, 7, 7, 2, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 2, 7, 7, 7, 2, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 7, 7, 7, 7, 2, 7, 2, 0, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 7, 2, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 2, 7, 2, 2, 2, 1, 0, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4, 7, 4, 4, 4, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 4, 7, 7, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 2, 2, 2, 7, 7, 7, 2, 7, 7, 7, 7, 2, 7, 2, 7, 7, 2, 7, 7, 7, 7, 4, 4, 4, 4, 4, 7, 7, 2, 7, 2, 7, 7, 7, 2, 7, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 8, 7, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 2, 7, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 2, 7, 2, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 2, 7, 7, 7, 2, 2, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 4, 2, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 7, 4, 4, 4, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 0, 7, 7, 7, 4, 4, 4, 4, 7, 4, 8, 7, 7, 7, 7, 4, 4, 1, 6, 6, 0, 6, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 4, 8, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 2, 4, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 8, 7, 7, 4, 4, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 8, 7, 2, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 1, 7, 7, 7, 7, 1, 1, 8, 7, 1, 8, 7, 7, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0, 1, 1, 0, 0, 7, 7, 7, 7, 7, 0, 0, 1, 0, 7, 7, 0, 0, 0, 0, 7, 7, 7, 7, 0, 0, 0, 0, 7, 7, 6, 7, 0, 0, 0, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 1, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 8, 4, 7, 1, 1, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 8, 4, 4, 7, 4, 4, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 8, 7, 8, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 4, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 4, 4, 4, 4, 7, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 4, 4, 4, 8, 4, 4, 4, 4, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 4, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 8, 7, 4, 7, 4, 4, 4, 4, 7, 4, 4, 7, 4, 4, 4, 7, 7, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 4, 4, 4, 4, 4, 8, 4, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 4, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 4, 4, 4, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 8, 7, 4, 7, 7, 4, 4, 4, 7, 8, 4, 4, 4, 7, 4, 7, 7, 8, 7, 8, 4, 4, 7, 7, 4, 4, 4, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 4, 4, 8, 7, 4, 7, 7, 7, 8, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 8, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 4, 8, 4, 4, 4, 4, 8, 4, 7, 4, 4, 7, 4, 7, 7, 7, 7, 1, 1, 1, 7, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 4, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 8, 7, 4, 7, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 8, 2, 7, 7, 4, 4, 7, 7, 7, 7, 4, 4, 7, 7, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 4, 4, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 4, 4, 4, 8, 7, 7, 7, 4, 4, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 4, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 2, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 8, 7, 4, 4, 7, 4, 7, 8, 4, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 4, 4, 7, 4, 7, 8, 7, 7, 4, 4, 4, 7, 8, 4, 4, 7, 7, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 4, 4, 7, 4, 7, 7, 7, 7, 4, 4, 7, 7, 7, 4, 4, 7, 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 7, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 4, 7, 8, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 4, 7, 8, 4, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 8, 8, 4, 4, 4, 7, 8, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 8, 7, 7, 7, 7, 4, 4, 7, 7, 4, 8, 7, 4, 8, 4, 4, 4, 7, 1, 7, 7, 7, 1, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 8, 7, 7, 7, 7, 8, 4, 4, 7, 4, 7, 7, 7, 4, 4, 8, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 4, 4, 7, 7, 4, 4, 8, 4, 4, 4, 4, 4, 4, 7, 4, 7, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 8, 7, 7, 4, 4, 7, 4, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 4, 4, 4, 4, 4, 4, 8, 4, 4, 4, 7, 8, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 1, 1, 1, 7, 1, 8, 7, 7, 7, 7, 1, 1, 1, 7, 1, 7, 7, 7, 4, 4, 7, 7, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 4, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 4, 4, 4, 7, 7, 8, 7, 7, 7, 4, 4, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 8, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 2, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 4, 4, 7, 4, 4, 7, 4, 7, 4, 4, 7, 4, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 4, 7, 7, 4, 4, 7, 4, 4, 7, 4, 4, 4, 7, 4, 7, 7, 7, 7, 4, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 4, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 7, 4, 4, 4, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 4, 7, 4, 8, 4, 4, 4, 4, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 1, 1, 7, 7, 7, 7, 8, 4, 4, 4, 4, 8, 7, 7, 4, 8, 4, 4, 4, 8, 4, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 2, 7, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 4, 4, 7, 7, 4, 4, 4, 4, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 4, 4, 4, 4, 4, 4, 4, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 4, 4, 7, 7, 7, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 4, 7, 8, 7, 4, 4, 7, 2, 2, 4, 7, 4, 7, 4, 4, 4, 7, 4, 4, 7, 7, 4, 7, 4, 4, 4, 7, 4, 4, 7, 4, 7, 4, 4, 4, 4, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 4, 4, 4, 4, 4, 7, 2, 2, 7, 4, 7, 4, 4, 4, 4, 4, 7, 4, 4, 4, 7, 4, 7, 7, 7, 7, 7, 4, 4, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 4, 7, 7, 7, 4, 7, 4, 4, 7, 7, 4, 7, 4, 8, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 4, 7, 7, 7, 7, 4, 7, 4, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 1, 1, 1, 7, 1, 8, 7, 1, 1, 7, 7, 7, 7, 7, 1, 1, 1, 7, 7, 7, 2, 1, 7, 1, 7, 1, 1, 1, 7, 1, 7, 1, 1, 7, 1, 1, 1, 1, 8, 7, 7, 7, 2, 7, 7, 1, 1, 7, 8, 8, 1, 7, 7, 7, 1, 7, 7, 7, 7, 7, 8, 7, 8, 7, 1, 7, 1, 1, 1, 8, 7, 8, 8, 8, 8, 7, 7, 8, 8, 7, 7, 8, 8, 8, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 8, 8, 8, 7, 8, 8, 8, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 8, 8, 8, 7, 7, 8, 8, 8, 7, 7, 7, 7, 7, 7, 8, 7, 8, 7, 2, 8, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 2, 2, 7, 7, 7, 7, 7, 8, 8, 8, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 8, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 8, 8, 7, 7, 7, 8, 8, 8, 7, 7, 7, 7, 7, 7, 8, 7, 2, 2, 8, 7, 7, 7, 8, 8, 7, 7, 7, 7, 8, 7, 8, 8, 8, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 7, 8, 8, 8, 7, 8, 7, 7, 8, 7, 8, 8, 7, 7, 7, 7, 7, 8, 2, 2, 7, 8, 7, 8, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 8, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 8, 7, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 8, 7, 1, 1, 1, 1, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 7, 7, 8, 7, 7, 8, 1, 8, 8, 1, 8, 7, 7, 7, 7, 7, 1, 7, 1, 1, 1, 8, 2, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 1, 7, 7, 7, 7, 10, 7, 7, 7, 7, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 2, 7, 7, 7, 7, 0, 7, 7, 7, 2, 7, 7, 0, 2, 7, 7, 7, 7, 7, 8, 2, 2, 7, 2, 7, 7, 7, 7, 7, 7, 2, 2, 7, 2, 7, 7, 2, 7, 2, 7, 7, 7, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 2, 2, 2, 2, 7, 7, 7, 2, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 7, 7, 7, 8, 2, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 2, 1, 7, 7, 7, 2, 2, 7, 2, 2, 8, 2, 7, 7, 7, 8, 2, 2, 2, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 2, 2, 2, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 7, 7, 8, 8, 8, 7, 7, 8, 8, 8, 8, 7, 7, 8, 8, 8, 7, 7, 7, 8, 8, 8, 7, 2, 7, 8, 7, 7, 7, 8, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 8, 7, 7, 8, 8, 7, 8, 8, 8, 7, 8, 8, 8, 8, 8, 7, 8, 8, 7, 7, 7, 8, 8, 8, 8, 7, 8, 8, 8, 7, 8, 8, 1, 8, 8, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 7, 7, 8, 7, 7, 7, 7, 7, 8, 8, 7, 8, 7, 7, 1, 7, 7, 8, 8, 8, 8, 7, 8, 8, 8, 7, 7, 7, 8, 7, 8, 8, 7, 8, 7, 7, 7, 7, 8, 8, 8, 8, 7, 8, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 8, 8, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 7, 7, 8, 8, 8, 8, 8, 7, 7, 1, 7, 8, 8, 7, 7, 8, 7, 8, 8, 7, 7, 8, 8, 8, 8, 7, 8, 8, 8, 8, 7, 7, 7, 8, 8, 8, 8, 7, 7, 8, 8, 8, 2, 7, 7, 7, 8, 7, 8, 8, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 8, 8, 7, 7, 7, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 8, 8, 7, 7, 7, 7, 8, 8, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 7, 8, 7, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 7, 7, 8, 7, 8, 8, 8, 8, 7, 8, 7, 7, 7, 7, 8, 7, 7, 8, 8, 7, 7, 8, 8, 8, 8, 7, 8, 8, 8, 7, 8, 8, 8, 7, 7, 7, 8, 8, 8, 8, 8, 7, 8, 8, 7, 8, 7, 7, 8, 8, 8, 7, 8, 7, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 8, 7, 7, 8, 7, 7, 8, 8, 7, 8, 8, 8, 8, 7, 8, 8, 8, 7, 7, 8, 8, 7, 2, 7, 8, 7, 8, 8, 8, 8, 8, 8, 8, 7, 8, 7, 7, 7, 8, 8, 1, 8, 8, 8, 7, 8, 8, 8, 7, 8, 7, 7, 7, 7, 7, 1, 1, 7, 8, 8, 8, 7, 2, 2, 8, 7, 7, 7, 7, 7, 1, 7, 8, 8, 8, 1, 7, 8, 8, 8, 8, 2, 7, 7, 7, 7, 7, 7, 7, 7, 10, 10, 7, 7, 4, 4, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 4, 4, 7, 4, 4, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 4, 4, 4, 7, 7, 7, 7, 4, 4, 7, 7, 4, 7, 4, 4, 7, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 4, 7, 7, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 2, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 7, 7, 7, 4, 4, 4, 4, 7, 7, 4, 4, 4, 7, 4, 4, 7, 8, 7, 7, 7, 7, 4, 4, 4, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 7, 4, 4, 7, 4, 4, 7, 8, 4, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 4, 4, 7, 4, 7, 6, 7, 7, 7, 7, 7, 7, 8, 4, 4, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 7, 7, 7, 4, 7, 4, 4, 4, 4, 4, 4, 7, 4, 7, 7, 7, 7, 7, 4, 2, 4, 7, 7, 4, 7, 4, 4, 7, 7, 4, 7, 4, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 4, 4, 7, 4, 7, 7, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 4, 4, 4, 8, 4, 7, 4, 4, 4, 7, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 1, 7, 7, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 8, 4, 8, 7, 7, 7, 7, 7, 7, 8, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 4, 7, 7, 4, 4, 7, 4, 7, 8, 4, 4, 4, 4, 7, 8, 4, 4, 4, 7, 7, 4, 7, 4, 7, 4, 0, 4, 7, 4, 7, 7, 7, 4, 7, 4, 4, 8, 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 7, 4, 7, 4, 7, 4, 7, 4, 4, 4, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 8, 4, 7, 7, 7, 8, 4, 4, 4, 4, 8, 4, 4, 4, 4, 7, 4, 7, 7, 7, 7, 4, 4, 4, 7, 4, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 8, 1, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted category id for selected test dataset.\n",
    "q = []\n",
    "for p in output:\n",
    "  q.append(p.cpu().numpy().tolist())\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "vnJiHq5Lm5u5"
   },
   "outputs": [],
   "source": [
    "# getting the actual labels\n",
    "p = new_df_test['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10508, 10508)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p), len(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "lqrt7A_W8miu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract     0.7158    0.4114    0.5225       508\n",
      "      author     0.1099    0.3818    0.1707        55\n",
      "     caption     0.1508    0.3577    0.2121       137\n",
      "        date     0.0000    0.0000    0.0000         9\n",
      "    equation     0.5032    0.4833    0.4930      1136\n",
      "      figure     1.0000    0.1667    0.2857         6\n",
      "      footer     0.0000    0.0000    0.0000        18\n",
      "   paragraph     0.8327    0.8281    0.8304      7965\n",
      "   reference     0.2250    0.2358    0.2303       581\n",
      "     section     0.0000    0.0000    0.0000        50\n",
      "       title     0.0000    0.0000    0.0000        43\n",
      "\n",
      "    accuracy                         0.7196     10508\n",
      "   macro avg     0.3216    0.2604    0.2495     10508\n",
      "weighted avg     0.7358    0.7196    0.7246     10508\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "target_names = list(integer_mapping.keys())\n",
    "report = classification_report(p,q, digits=4, target_names=target_names)\n",
    "matrix = confusion_matrix(p,q)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    abstract     0.7158    0.4114    0.5225       508\n",
      "      author     0.1099    0.3818    0.1707        55\n",
      "     caption     0.1508    0.3577    0.2121       137\n",
      "        date     0.0000    0.0000    0.0000         9\n",
      "    equation     0.5032    0.4833    0.4930      1136\n",
      "      figure     1.0000    0.1667    0.2857         6\n",
      "      footer     0.0000    0.0000    0.0000        18\n",
      "   paragraph     0.8327    0.8281    0.8304      7965\n",
      "   reference     0.2250    0.2358    0.2303       581\n",
      "     section     0.0000    0.0000    0.0000        50\n",
      "       title     0.0000    0.0000    0.0000        43\n",
      "\n",
      "    accuracy                         0.7196     10508\n",
      "   macro avg     0.3216    0.2604    0.2495     10508\n",
      "weighted avg     0.7358    0.7196    0.7246     10508\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "target_names = list(integer_mapping.keys())\n",
    "report = classification_report(p,q, digits=4, target_names=target_names)\n",
    "matrix = confusion_matrix(p,q)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torcheval\n",
      "  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m255.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting torchtnt>=0.0.5\n",
      "  Downloading torchtnt-0.1.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m421.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torcheval) (4.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (67.6.1)\n",
      "Collecting pyre-extensions\n",
      "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: fsspec in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (2023.4.0)\n",
      "Requirement already satisfied: packaging in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (23.0)\n",
      "Requirement already satisfied: torch in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (4.65.0)\n",
      "Requirement already satisfied: psutil in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (5.9.4)\n",
      "Requirement already satisfied: tensorboard in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/tensorboard-2.12.2-py3.9.egg (from torchtnt>=0.0.5->torcheval) (2.12.2)\n",
      "Requirement already satisfied: numpy in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torchtnt>=0.0.5->torcheval) (1.23.5)\n",
      "Requirement already satisfied: typing-inspect in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from pyre-extensions->torchtnt>=0.0.5->torcheval) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/absl_py-1.4.0-py3.9.egg (from tensorboard->torchtnt>=0.0.5->torcheval) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.53.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.17.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/google_auth_oauthlib-1.0.0-py3.9.egg (from tensorboard->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/Markdown-3.4.3-py3.9.egg (from tensorboard->torchtnt>=0.0.5->torcheval) (3.4.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/tensorboard_data_server-0.7.0-py3.9.egg (from tensorboard->torchtnt>=0.0.5->torcheval) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/tensorboard_plugin_wit-1.8.1-py3.9.egg (from tensorboard->torchtnt>=0.0.5->torcheval) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.40.0)\n",
      "Requirement already satisfied: filelock in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torch->torchtnt>=0.0.5->torcheval) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torch->torchtnt>=0.0.5->torcheval) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/requests_oauthlib-1.3.1-py3.9.egg (from google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->torchtnt>=0.0.5->torcheval) (6.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->torchtnt>=0.0.5->torcheval) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from sympy->torch->torchtnt>=0.0.5->torcheval) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from typing-inspect->pyre-extensions->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->torchtnt>=0.0.5->torcheval) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/shashanksahoo/miniconda3/envs/gpt-LLM/lib/python3.9/site-packages/oauthlib-3.2.2-py3.9.egg (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (3.2.2)\n",
      "Installing collected packages: pyre-extensions, torchtnt, torcheval\n",
      "Successfully installed pyre-extensions-0.0.30 torcheval-0.0.6 torchtnt-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7196)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "multiclass_f1_score(torch.tensor(p), torch.tensor(q), num_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQpJTV7RSVZW",
    "cJjfftEpSkfm",
    "B7TWqvbTppX_",
    "RbiQjAgdSqhn"
   ],
   "machine_shape": "hm",
   "name": "Funsd_Object_Detection_bert_large.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
