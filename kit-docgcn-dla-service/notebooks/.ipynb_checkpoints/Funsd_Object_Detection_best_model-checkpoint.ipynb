{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OhO4xlwqExT"
      },
      "source": [
        "# Publaynet BERT Classifier\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQpJTV7RSVZW"
      },
      "source": [
        "## Environment Setup\n",
        "Import key libraries and working envorinments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-GlywkSFegL"
      },
      "source": [
        "!pip install transformers==4.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "979OUro5Eac3"
      },
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "#from transformers.configuration_bert import BertConfig\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd2-rgBQp269"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmPHwOWpdCYg"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "  global drive\n",
        "  \n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "  authenticate()\n",
        "  \n",
        "  for fileId in fileIds:    \n",
        "    \n",
        "    downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "    downloaded.GetContentFile(fileId[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TWqvbTppX_"
      },
      "source": [
        "##Get Training and Validation Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BLhhxBCqwZd"
      },
      "source": [
        "#Do not downloading training and validation dataset at same time \n",
        "try:\n",
        "  _ = open(\"testing_dataset.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"testing_dataset.pkl\", \"1fktW64hxcjCXreMTv_2pAxSe4Nt083z3\"]])\n",
        "\n",
        "try:\n",
        "  _ = open(\"training_dataset.pkl\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"training_dataset.pkl\", \"1td4mF-QxrwKF125xR5DWflGqcwn0z1LP\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_pickle('training_dataset.pkl')\n",
        "df_test = pd.read_pickle('testing_dataset.pkl')"
      ],
      "metadata": {
        "id": "cU_KF43l9UcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB8gHoFVObjC"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb1Q5N6LGK7z"
      },
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk1wrbvciXrg"
      },
      "source": [
        "df_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baSmeDdIEadM"
      },
      "source": [
        "new_df = df_train[['text', 'label','near_visual_feature',\t'gcn_near_char_density',\t'gcn_near_char_number',\n",
        "                   'level1_parse_emb','level2_parse_emb','gcn_near_token_density','density','visual_feature','gcn_bert_predicted']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX1g2eqC0s4I"
      },
      "source": [
        "new_df_test = df_test[['text', 'label','near_visual_feature',\t'gcn_near_char_density',\t'gcn_near_char_number',\n",
        "                   'level1_parse_emb','level2_parse_emb','gcn_near_token_density','density','visual_feature','gcn_bert_predicted']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbiQjAgdSqhn"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvXxpfNCGER2"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 100\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = TRAIN_BATCH_SIZE*2\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 2e-05\n",
        "# Change the pre-trained bert model\n",
        "#tokenizer = BertTokenizer.from_pretrained('roberta-base') #Cased "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vWRDemOGxJD"
      },
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.label\n",
        "        self.gcn_visual_feature = dataframe.near_visual_feature\n",
        "        self.visual_feature = dataframe.visual_feature\n",
        "        self.gcn_bert_base = dataframe.gcn_bert_predicted\n",
        "        self.parsing1 = dataframe.level1_parse_emb\n",
        "        self.parsing2 = dataframe.level2_parse_emb\n",
        "        self.char_density = dataframe.gcn_near_char_density\n",
        "        self.char_number = dataframe.gcn_near_char_number\n",
        "        self.density = dataframe.density\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
        "            'density': torch.tensor(self.density[index],dtype=torch.float),\n",
        "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
        "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
        "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
        "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
        "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
        "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
        "            'gcn_visual_feature': torch.tensor(self.gcn_visual_feature[index],dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gpe9D1QHoCd"
      },
      "source": [
        "train_size = 1\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\n",
        "#test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(new_df_test.shape))\n",
        "\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "#testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
        "test_set = SentimentData(new_df_test,tokenizer,MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1tInLk2Eadt"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "#testing_loader = DataLoader(testing_set, **test_params)\n",
        "vali_loader = DataLoader(test_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF12YgfxSwEr"
      },
      "source": [
        "## Define the proposed classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMqQTafXEaei"
      },
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        #bert-base-cased 768\n",
        "        #bert-large-cased bert-large-uncased 1024\n",
        "        #roberta-base-cased 768\n",
        "        #biobert\n",
        "\n",
        "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\")# BERT large\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.hidden_cls = torch.nn.Linear(768,768)\n",
        "        self.hidden_parsing = torch.nn.Linear(768,768)\n",
        "        self.hidden_den = torch.nn.Linear(768,768)\n",
        "        self.hidden_vis = torch.nn.Linear(768,768)\n",
        "        self.hidden_vis_pro = torch.nn.Linear(768,768)\n",
        "        self.hidden_all = torch.nn.Linear(768*2,768*2)\n",
        "        self.before_classifier = torch.nn.Linear(768*2,128)\n",
        "\\\n",
        "        self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
        "        self.classifier = torch.nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, char_density,char_number,visual_feature,bert_cls,parsing1,parsing2,visual):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "\n",
        "        # BERT 768 BERT / large 1024\n",
        "        \n",
        "        # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
        "        \n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        pooler = torch.cat((pooler.unsqueeze(1),bert_cls.unsqueeze(1)),1)\n",
        "        pooler = self.pooling(pooler).squeeze(1)\n",
        "        pooler = self.hidden_cls(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        visual = self.hidden_vis_pro(visual)\n",
        "        visual = torch.nn.Tanh()(visual)\n",
        "        visual = self.dropout(visual)\n",
        "\n",
        "        visual = torch.cat((visual.unsqueeze(1),visual_feature.unsqueeze(1)),1)\n",
        "        visual = self.pooling(visual).squeeze(1)\n",
        "        visual = self.hidden_vis(visual)\n",
        "        visual = torch.nn.Tanh()(visual)\n",
        "        visual = self.dropout(visual)\n",
        "\n",
        "        pooler = torch.cat((pooler,visual),1)\n",
        "        pooler = self.hidden_all(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        pooler = self.before_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ55mIPZIkp_"
      },
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XBJjGKdS2b8"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZ7YuJ5InOS"
      },
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-05) # change learning rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPhA2V3iIpzN"
      },
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhqvtY2SIup7"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    output = []\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "        visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "        gcn_visual_feature = data['gcn_visual_feature'].to(device, dtype = torch.float)\n",
        "        gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
        "        parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "        parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "        char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "        char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids,char_density,char_number,gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ecydbX_S6Lg"
      },
      "source": [
        "## Validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFiNcy16JLwt"
      },
      "source": [
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    output_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            char_den = data['char_density'].to(device, dtype = torch.float)\n",
        "            density = data['density'].to(device, dtype = torch.float)\n",
        "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
        "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "            token_density = data['token_density'].to(device, dtype = torch.float)\n",
        "            token_number = data['token_number'].to(device, dtype = torch.float)\n",
        "\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids,visual_feature,char_density,char_number,token_density,token_number).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            output_list = output_list + list(big_idx)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu,output_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-SBT2HeR_5t"
      },
      "source": [
        "acc,pre_list = valid(model, vali_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8lSc-jf7bU"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qD3e22vE7My"
      },
      "source": [
        "class SentimentData_test(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.label\n",
        "        self.gcn_visual_feature = dataframe.near_visual_feature\n",
        "        self.visual_feature = dataframe.visual_feature\n",
        "        self.gcn_bert_base = dataframe.gcn_bert_predicted\n",
        "        self.parsing1 = dataframe.level1_parse_emb\n",
        "        self.parsing2 = dataframe.level2_parse_emb\n",
        "        self.char_density = dataframe.gcn_near_char_density\n",
        "        self.char_number = dataframe.gcn_near_char_number\n",
        "        self.density = dataframe.density\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
        "            'density': torch.tensor(self.density[index],dtype=torch.float),\n",
        "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
        "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
        "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
        "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
        "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
        "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
        "            'gcn_visual_feature': torch.tensor(self.gcn_visual_feature[index],dtype=torch.float),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMRN5w3RXjqB"
      },
      "source": [
        "### load the test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaw4D5UKFe-Z"
      },
      "source": [
        "new_df_true_test = new_df_test[['text', 'label','near_visual_feature',\t'gcn_near_char_density',\t'gcn_near_char_number',\n",
        "                   'level1_parse_emb','level2_parse_emb','gcn_bert_predicted','visual_feature','gcn_near_token_density','density']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0kUqjR1Fhb6"
      },
      "source": [
        "test = SentimentData_test(new_df_true_test,tokenizer, MAX_LEN)\n",
        "testing_loader = DataLoader(test, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IetKrn_SY-OT"
      },
      "source": [
        "def test_label_generator(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    output_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
        "            gcn_visual_feature = data['gcn_visual_feature'].to(device, dtype = torch.float)\n",
        "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
        "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
        "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
        "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
        "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids,char_density,char_number,gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature).squeeze()\n",
        "\n",
        "            \n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            output_list = output_list + list(big_idx)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            \n",
        "    return output_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4\n",
        "for epoch in range(EPOCHS):\n",
        "  train(epoch)\n",
        "  output = test_label_generator(model, testing_loader)\n",
        "  q = []\n",
        "  for p in output:\n",
        "    q.append(p.cpu().numpy().tolist())\n",
        "  p = new_df_test['label'].tolist()\n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  report = classification_report(p,q, digits=4)\n",
        "  print(report)  "
      ],
      "metadata": {
        "id": "jZTbJkdtWoZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}