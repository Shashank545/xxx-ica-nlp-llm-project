{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OhO4xlwqExT"
   },
   "source": [
    "# Publaynet BERT Classifier\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQpJTV7RSVZW"
   },
   "source": [
    "## Environment Setup\n",
    "Import key libraries and working envorinments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-GlywkSFegL"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "979OUro5Eac3"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#from transformers.configuration_bert import BertConfig\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd2-rgBQp269"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmPHwOWpdCYg"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import pandas as pd\n",
    "\n",
    "# Authenticate\n",
    "drive = None\n",
    "def authenticate():\n",
    "  global drive\n",
    "  \n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "\n",
    "#Download files\n",
    "def downloadFiles(fileIds):\n",
    "  authenticate()\n",
    "  \n",
    "  for fileId in fileIds:    \n",
    "    \n",
    "    downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
    "    downloaded.GetContentFile(fileId[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7TWqvbTppX_"
   },
   "source": [
    "##Get Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BLhhxBCqwZd"
   },
   "outputs": [],
   "source": [
    "#Do not downloading training and validation dataset at same time \n",
    "try:\n",
    "  _ = open(\"testing_dataset.pkl\", \"r\")\n",
    "except:\n",
    "  downloadFiles([[\"testing_dataset.pkl\", \"1fktW64hxcjCXreMTv_2pAxSe4Nt083z3\"]])\n",
    "\n",
    "try:\n",
    "  _ = open(\"training_dataset.pkl\", \"r\")\n",
    "except:\n",
    "  downloadFiles([[\"training_dataset.pkl\", \"1td4mF-QxrwKF125xR5DWflGqcwn0z1LP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cU_KF43l9UcS"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('training_dataset.pkl')\n",
    "df_test = pd.read_pickle('testing_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB8gHoFVObjC"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb1Q5N6LGK7z"
   },
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk1wrbvciXrg"
   },
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baSmeDdIEadM"
   },
   "outputs": [],
   "source": [
    "new_df = df_train[['text', 'label','near_visual_feature',\t'gcn_near_char_density',\t'gcn_near_char_number',\n",
    "                   'level1_parse_emb','level2_parse_emb','gcn_near_token_density','density','visual_feature','gcn_bert_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qX1g2eqC0s4I"
   },
   "outputs": [],
   "source": [
    "new_df_test = df_test[['text', 'label','near_visual_feature',\t'gcn_near_char_density',\t'gcn_near_char_number',\n",
    "                   'level1_parse_emb','level2_parse_emb','gcn_near_token_density','density','visual_feature','gcn_bert_predicted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbiQjAgdSqhn"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvXxpfNCGER2"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = TRAIN_BATCH_SIZE*2\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 2e-05\n",
    "# Change the pre-trained bert model\n",
    "#tokenizer = BertTokenizer.from_pretrained('roberta-base') #Cased "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vWRDemOGxJD"
   },
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.label\n",
    "        self.gcn_visual_feature = dataframe.near_visual_feature\n",
    "        self.visual_feature = dataframe.visual_feature\n",
    "        self.gcn_bert_base = dataframe.gcn_bert_predicted\n",
    "        self.parsing1 = dataframe.level1_parse_emb\n",
    "        self.parsing2 = dataframe.level2_parse_emb\n",
    "        self.char_density = dataframe.gcn_near_char_density\n",
    "        self.char_number = dataframe.gcn_near_char_number\n",
    "        self.density = dataframe.density\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
    "            'density': torch.tensor(self.density[index],dtype=torch.float),\n",
    "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
    "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
    "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
    "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
    "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
    "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
    "            'gcn_visual_feature': torch.tensor(self.gcn_visual_feature[index],dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Gpe9D1QHoCd"
   },
   "outputs": [],
   "source": [
    "train_size = 1\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "#test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(new_df_test.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "#testing_set = SentimentData(test_data, tokenizer, MAX_LEN)\n",
    "test_set = SentimentData(new_df_test,tokenizer,MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1tInLk2Eadt"
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "#testing_loader = DataLoader(testing_set, **test_params)\n",
    "vali_loader = DataLoader(test_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jF12YgfxSwEr"
   },
   "source": [
    "## Define the proposed classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMqQTafXEaei"
   },
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        #bert-base-cased 768\n",
    "        #bert-large-cased bert-large-uncased 1024\n",
    "        #roberta-base-cased 768\n",
    "        #biobert\n",
    "\n",
    "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\")# BERT large\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.hidden_cls = torch.nn.Linear(768,768)\n",
    "        self.hidden_parsing = torch.nn.Linear(768,768)\n",
    "        self.hidden_den = torch.nn.Linear(768,768)\n",
    "        self.hidden_vis = torch.nn.Linear(768,768)\n",
    "        self.hidden_vis_pro = torch.nn.Linear(768,768)\n",
    "        self.hidden_all = torch.nn.Linear(768*2,768*2)\n",
    "        self.before_classifier = torch.nn.Linear(768*2,128)\n",
    "\\\n",
    "        self.pooling = torch.nn.MaxPool2d((2,1), stride=None)\n",
    "        self.classifier = torch.nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, char_density,char_number,visual_feature,bert_cls,parsing1,parsing2,visual):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "\n",
    "        # BERT 768 BERT / large 1024\n",
    "        \n",
    "        # set different hidden layer, number of hidden units, regularization methods including bn and dropout\n",
    "        \n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        pooler = torch.cat((pooler.unsqueeze(1),bert_cls.unsqueeze(1)),1)\n",
    "        pooler = self.pooling(pooler).squeeze(1)\n",
    "        pooler = self.hidden_cls(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        visual = self.hidden_vis_pro(visual)\n",
    "        visual = torch.nn.Tanh()(visual)\n",
    "        visual = self.dropout(visual)\n",
    "\n",
    "        visual = torch.cat((visual.unsqueeze(1),visual_feature.unsqueeze(1)),1)\n",
    "        visual = self.pooling(visual).squeeze(1)\n",
    "        visual = self.hidden_vis(visual)\n",
    "        visual = torch.nn.Tanh()(visual)\n",
    "        visual = self.dropout(visual)\n",
    "\n",
    "        pooler = torch.cat((pooler,visual),1)\n",
    "        pooler = self.hidden_all(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        pooler = self.before_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ55mIPZIkp_"
   },
   "outputs": [],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XBJjGKdS2b8"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYZ7YuJ5InOS"
   },
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-05) # change learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPhA2V3iIpzN"
   },
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhqvtY2SIup7"
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    output = []\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
    "        gcn_visual_feature = data['gcn_visual_feature'].to(device, dtype = torch.float)\n",
    "        gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
    "        parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
    "        parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
    "        char_density = data['char_density'].to(device, dtype = torch.float)\n",
    "        char_number = data['char_number'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids,char_density,char_number,gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ecydbX_S6Lg"
   },
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFiNcy16JLwt"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            char_den = data['char_density'].to(device, dtype = torch.float)\n",
    "            density = data['density'].to(device, dtype = torch.float)\n",
    "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
    "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
    "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
    "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
    "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
    "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
    "            token_density = data['token_density'].to(device, dtype = torch.float)\n",
    "            token_number = data['token_number'].to(device, dtype = torch.float)\n",
    "\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids,visual_feature,char_density,char_number,token_density,token_number).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            output_list = output_list + list(big_idx)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu,output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-SBT2HeR_5t"
   },
   "outputs": [],
   "source": [
    "acc,pre_list = valid(model, vali_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_8lSc-jf7bU"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qD3e22vE7My"
   },
   "outputs": [],
   "source": [
    "class SentimentData_test(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.label\n",
    "        self.gcn_visual_feature = dataframe.near_visual_feature\n",
    "        self.visual_feature = dataframe.visual_feature\n",
    "        self.gcn_bert_base = dataframe.gcn_bert_predicted\n",
    "        self.parsing1 = dataframe.level1_parse_emb\n",
    "        self.parsing2 = dataframe.level2_parse_emb\n",
    "        self.char_density = dataframe.gcn_near_char_density\n",
    "        self.char_number = dataframe.gcn_near_char_number\n",
    "        self.density = dataframe.density\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
    "            'density': torch.tensor(self.density[index],dtype=torch.float),\n",
    "            'gcn_bert_base': torch.tensor(self.gcn_bert_base[index],dtype=torch.float),\n",
    "            'char_density': torch.tensor(self.char_density[index],dtype=torch.float),\n",
    "            'char_number': torch.tensor(self.char_number[index],dtype=torch.float),\n",
    "            'visual_feature': torch.tensor(self.visual_feature[index],dtype=torch.float),\n",
    "            'parsing1': torch.tensor(self.parsing1[index],dtype=torch.float),\n",
    "            'parsing2': torch.tensor(self.parsing2[index],dtype=torch.float),\n",
    "            'gcn_visual_feature': torch.tensor(self.gcn_visual_feature[index],dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMRN5w3RXjqB"
   },
   "source": [
    "### load the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zaw4D5UKFe-Z"
   },
   "outputs": [],
   "source": [
    "new_df_true_test = new_df_test[['text', 'label','near_visual_feature',\t'gcn_near_char_density',\t'gcn_near_char_number',\n",
    "                   'level1_parse_emb','level2_parse_emb','gcn_bert_predicted','visual_feature','gcn_near_token_density','density']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0kUqjR1Fhb6"
   },
   "outputs": [],
   "source": [
    "test = SentimentData_test(new_df_true_test,tokenizer, MAX_LEN)\n",
    "testing_loader = DataLoader(test, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IetKrn_SY-OT"
   },
   "outputs": [],
   "source": [
    "def test_label_generator(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    output_list = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            visual_feature = data['visual_feature'].to(device, dtype = torch.float)\n",
    "            gcn_visual_feature = data['gcn_visual_feature'].to(device, dtype = torch.float)\n",
    "            gcn_bert_base = data['gcn_bert_base'].to(device, dtype = torch.float)\n",
    "            parsing1 = data['parsing1'].to(device, dtype = torch.float)\n",
    "            parsing2 = data['parsing2'].to(device, dtype = torch.float)\n",
    "            char_density = data['char_density'].to(device, dtype = torch.float)\n",
    "            char_number = data['char_number'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids,char_density,char_number,gcn_visual_feature,gcn_bert_base,parsing1,parsing2,visual_feature).squeeze()\n",
    "\n",
    "            \n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            output_list = output_list + list(big_idx)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZTbJkdtWoZZ"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "for epoch in range(EPOCHS):\n",
    "  train(epoch)\n",
    "  output = test_label_generator(model, testing_loader)\n",
    "  q = []\n",
    "  for p in output:\n",
    "    q.append(p.cpu().numpy().tolist())\n",
    "  p = new_df_test['label'].tolist()\n",
    "  from sklearn.metrics import classification_report, confusion_matrix\n",
    "  report = classification_report(p,q, digits=4)\n",
    "  print(report)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
